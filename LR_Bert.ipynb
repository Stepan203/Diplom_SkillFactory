{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SCAN (https://scan-interfax.ru/) - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –≥—Ä—É–ø–ø—ã \"–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å\", –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –≤ —Å—Ñ–µ—Ä–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–µ–ø—É—Ç–∞—Ü–∏–µ–π –∏ –∞–Ω–∞–ª–∏–∑–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ PR. –†–µ—à–∞–µ—Ç –∑–∞–¥–∞—á–∏ –ø–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥—É –°–ú–ò –∏ —Å–æ—Ü—Å–µ—Ç–µ–π, –∞–Ω–∞–ª–∏–∑—É –º–µ–¥–∏–∞-–ø–æ–ª—è, –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–µ–ª–æ–≤–æ–π —Ä–µ–ø—É—Ç–∞—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–π –∏ –ø–µ—Ä—Å–æ–Ω, –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ —Ä–µ–∞–≥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –ø–æ—è–≤–ª–µ–Ω–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–∞ –≤ –°–ú–ò –∏ —Å–æ—Ü–º–µ–¥–∏–∞, –ø—É—Ç—ë–º –æ—Å—É—â–µ—Å—Ç–≤–ª–µ–Ω–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞ –ø—É–±–ª–∏—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –±–æ–ª–µ–µ —á–µ–º 60 —Ç—ã—Å—è—á –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.\n\n–û–¥–Ω–æ–π –∏–∑ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π SCAN —è–≤–ª—è–µ—Ç—Å—è —Ñ–∞–∫—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ - –≤—ã–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è —Å–æ–±—ã—Ç–∏–µ –∏–ª–∏ –¥–µ–π—Å—Ç–≤–∏–µ –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ —Å—É–±—ä–µ–∫—Ç–∞ –Ω–∞ –∑–∞–¥–∞–Ω–Ω—É—é —Ç–µ–º–∞—Ç–∏–∫—É –∏ —Å –∑–∞–¥–∞–Ω–Ω–æ–π —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é.\n–ù–∞ —Ç–µ–∫—É—â–∏–π –º–æ–º–µ–Ω—Ç SCAN —É–º–µ–µ—Ç –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –±–æ–ª–µ–µ —á–µ–º –ø–æ 800 —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ç–µ–º–∞–º. –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–π —Ç–µ–º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –ø–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º—É —Å–±–æ—Ä—É –∏ –∞–Ω–∞–ª–∏–∑—É –±–æ–ª—å—à–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã—Ö —Ñ—Ä–∞–∑, —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º –Ω–∞–ø–∏—Å–∞–Ω–∏–µ–º –º–∞—à–∏–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–º DSL (domain-specific language), –≤ –∫–æ—Ç–æ—Ä–æ–º –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω —Ü–µ–ª—ã–π –æ—Ç–¥–µ–ª –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö –ª–∏–Ω–≥–≤–∏—Å—Ç–æ–≤.\n\n–ú—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º –≤–∞–º –ø—Ä–∏–Ω—è—Ç—å —É—á–∞—Å—Ç–∏–µ –≤ —Ä–µ—à–µ–Ω–∏–∏ –∑–Ω–∞—á–∏–º–æ–π –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ SCAN –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–∏—Ç—å —Å–ø–µ–∫—Ç—Ä –≤—ã–¥–µ–ª—è–µ–º—ã—Ö —Å–∏—Å—Ç–µ–º–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –∏ —Å–Ω–∏–∑–∏—Ç –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ –ª–∏–Ω–≥–≤–∏—Å—Ç–æ–≤:\n\n–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å—Ä–µ–¥—Å—Ç–≤–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ –∑–∞–¥–∞–Ω–Ω—ã–µ —Ç–µ–º–∞—Ç–∏–∫–∏. –ù–∞–º –≤–∞–∂–Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –±–ª–∏–∑–æ—Å—Ç—å –∫ —É–∂–µ –ø—Ä–æ—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º –Ω–∞–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º, —á—Ç–æ–±—ã –Ω–µ —Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—å –æ–ø–∏—Å—ã–≤–∞—Ç—å –∫–∞–∂–¥—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–π —Ç–µ–º–∞—Ç–∏–∫–∏ –º–∞—à–∏–Ω–Ω—ã–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏ –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–º DSL:\n\n–í –∫–∞—á–µ—Å—Ç–≤–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤—ã—Å—Ç—É–ø–∞—é—Ç –Ω–∞–±–æ—Ä—ã —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–µ–º–∞—Ç–∏–∫–∏.\n–í –∫–∞—á–µ—Å—Ç–≤–µ –∏—Å–∫–æ–º–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–∂–µ—Ç –≤—ã—Å—Ç—É–ø–∞—Ç—å –∫–∞–∫ —á–∞—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, —Ç–∞–∫ –∏ —Ü–µ–ª–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –∏–ª–∏ –¥–∞–∂–µ –Ω–∞–±–æ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–∞ —Ç—É –∂–µ —Ç–µ–º—É.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstopwords_ru = stopwords.words(\"russian\")\nfrom nltk.stem import WordNetLemmatizer\n\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\nimport transformers\nimport torch\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer, BertModel, BertConfig","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:39:37.355579Z","iopub.execute_input":"2022-06-23T05:39:37.356364Z","iopub.status.idle":"2022-06-23T05:39:48.007409Z","shell.execute_reply.started":"2022-06-23T05:39:37.356219Z","shell.execute_reply":"2022-06-23T05:39:48.006308Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"## –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/scan-classification-challange/df_train.csv')\ntrain.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:39:48.009145Z","iopub.execute_input":"2022-06-23T05:39:48.009761Z","iopub.status.idle":"2022-06-23T05:39:48.685983Z","shell.execute_reply.started":"2022-06-23T05:39:48.009725Z","shell.execute_reply":"2022-06-23T05:39:48.684760Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/scan-classification-challange/df_test.csv', index_col=0)\ntest.sample(3)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:39:48.687458Z","iopub.execute_input":"2022-06-23T05:39:48.687882Z","iopub.status.idle":"2022-06-23T05:39:49.086464Z","shell.execute_reply.started":"2022-06-23T05:39:48.687847Z","shell.execute_reply":"2022-06-23T05:39:49.085483Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã\ntrain.drop_duplicates(subset={'text'}, inplace=True) \ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:39:49.088703Z","iopub.execute_input":"2022-06-23T05:39:49.089325Z","iopub.status.idle":"2022-06-23T05:39:49.215792Z","shell.execute_reply.started":"2022-06-23T05:39:49.089254Z","shell.execute_reply":"2022-06-23T05:39:49.214692Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/scan-classification-challange/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:39:49.216959Z","iopub.execute_input":"2022-06-23T05:39:49.217263Z","iopub.status.idle":"2022-06-23T05:39:49.246189Z","shell.execute_reply.started":"2022-06-23T05:39:49.217236Z","shell.execute_reply":"2022-06-23T05:39:49.245207Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train['encoded_cat'] = train['class'].astype('category').cat.codes\n#train.drop(['class'], inplace=True, axis=1)\ntrain.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:39:49.247597Z","iopub.execute_input":"2022-06-23T05:39:49.247908Z","iopub.status.idle":"2022-06-23T05:39:49.269058Z","shell.execute_reply.started":"2022-06-23T05:39:49.247878Z","shell.execute_reply":"2022-06-23T05:39:49.268104Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#train['sample'] = 1 # –ø–æ–º–µ—á–∞–µ–º –≥–¥–µ —É –Ω–∞—Å —Ç—Ä–µ–π–Ω\n#test['sample'] = 0 # –ø–æ–º–µ—á–∞–µ–º –≥–¥–µ —É –Ω–∞—Å —Ç–µ—Å—Ç\n#test['class'] = 0 # –≤ —Ç–µ—Å—Ç–µ —É –Ω–∞—Å –Ω–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è price, –º—ã –µ–≥–æ –¥–æ–ª–∂–Ω—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, –ø–æ—ç—Ç–æ–º—É –ø–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏\n\n#df = test.append(train, sort=False).reset_index(drop=True) # –æ–±—ä–µ–¥–∏–Ω—è–µ–º\n#print(train.shape, test.shape, df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T06:42:04.427314Z","iopub.execute_input":"2022-06-17T06:42:04.428061Z","iopub.status.idle":"2022-06-17T06:42:04.432238Z","shell.execute_reply.started":"2022-06-17T06:42:04.428021Z","shell.execute_reply":"2022-06-17T06:42:04.431434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    '''Text Preprocessing '''\n    \n    # Convert words to lower case\n    text = text.lower()\n    contractions = []\n    #Expand contractions\n    if True:\n        text = text.split()\n        new_text = []\n        for word in text:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n        text = \" \".join(new_text)\n    \n    # Format words and remove unwanted characters\n    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\<a href', ' ', text)\n    text = re.sub(r'&amp;', '', text) \n    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/0-9]', ' ', text)\n    #text = re.sub('[^A-Za-z0-9]+', ' ', text)\n    text = re.sub(r'<br />', ' ', text)\n    text = re.sub(r'\\'', ' ', text)\n    \n    # remove stopwords\n   # if remove_stopwords:\n    text = text.split()\n    stops = set(stopwords.words(\"russian\"))\n    text = [w for w in text if not w in stops]\n   # text = \" \".join(text)\n\n    # Tokenize each word\n    #text =  nltk.WordPunctTokenizer().tokenize(text)\n    \n    # Lemmatize each token\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    text = [lemmatizer.lemmatize(word) for word in text]\n   # lemm = nltk.stem.WordNetLemmatizer()\n    #text = list(map(lambda word:list(map(lemm.lemmatize, word)), text))\n    text = \" \".join(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:41:49.670451Z","iopub.execute_input":"2022-06-23T05:41:49.671448Z","iopub.status.idle":"2022-06-23T05:41:49.687218Z","shell.execute_reply.started":"2022-06-23T05:41:49.671258Z","shell.execute_reply":"2022-06-23T05:41:49.685875Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\"\"\"def clean_word(text):\n    '''Text Preprocessing '''\n    \n    # Convert words to lower case\n    text = text.lower()\n    contractions = []\n    #Expand contractions\n    if True:\n        text = text.split()\n        new_text = []\n        for word in text:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n        text = \" \".join(new_text)\n    \n    # Format words and remove unwanted characters\n    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\<a href', ' ', text)\n    text = re.sub(r'&amp;', '', text) \n    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/0-9]', ' ', text)\n    #text = re.sub('[^A-Za-z0-9]+', ' ', text)\n    text = re.sub(r'<br />', ' ', text)\n    text = re.sub(r'\\'', ' ', text)\n    \n    # remove stopwords\n   # if remove_stopwords:\n    text = text.split()\n    stops = set(stopwords.words(\"russian\"))\n    text = [w for w in text if not w in stops]\n   # text = \" \".join(text)\n\n    # Tokenize each word\n    #text =  nltk.WordPunctTokenizer().tokenize(text)\n    \n    # Lemmatize each token\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    text = [lemmatizer.lemmatize(word) for word in text]\n   # lemm = nltk.stem.WordNetLemmatizer()\n    #text = list(map(lambda word:list(map(lemm.lemmatize, word)), text))\n    text = \" \".join(text)\n    text = list(text)\n    text = \" \".join(text)\n    return text\n    \"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:41:06.193077Z","iopub.execute_input":"2022-06-23T05:41:06.193994Z","iopub.status.idle":"2022-06-23T05:41:06.206735Z","shell.execute_reply.started":"2022-06-23T05:41:06.193937Z","shell.execute_reply":"2022-06-23T05:41:06.205423Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#train['text_word'] = train['text'].apply(lambda x: clean_word(x))\n#train[['text_word', 'text']]","metadata":{"execution":{"iopub.status.busy":"2022-06-22T20:16:56.41072Z","iopub.execute_input":"2022-06-22T20:16:56.411146Z","iopub.status.idle":"2022-06-22T20:17:12.982688Z","shell.execute_reply.started":"2022-06-22T20:16:56.411112Z","shell.execute_reply":"2022-06-22T20:17:12.981639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text_Cleaned'] = train['text'].apply(lambda x: clean_text(x))\ntrain[['text_Cleaned', 'text']]\n#train['text_Cleaned'] = train['text'].apply(lambda x: clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:42:22.709175Z","iopub.execute_input":"2022-06-23T05:42:22.709666Z","iopub.status.idle":"2022-06-23T05:42:40.675779Z","shell.execute_reply.started":"2022-06-23T05:42:22.709627Z","shell.execute_reply":"2022-06-23T05:42:40.674781Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"test['text_Cleaned'] = test['text'].apply(lambda x: clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:42:40.677724Z","iopub.execute_input":"2022-06-23T05:42:40.678264Z","iopub.status.idle":"2022-06-23T05:42:50.999064Z","shell.execute_reply.started":"2022-06-23T05:42:40.678215Z","shell.execute_reply":"2022-06-23T05:42:50.997976Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Splitting data(80-20): Train | Test","metadata":{}},{"cell_type":"code","source":"y = train.encoded_cat.values     # –Ω–∞—à —Ç–∞—Ä–≥–µ—Ç\nX = train.drop(['encoded_cat', 'text'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:42:51.000543Z","iopub.execute_input":"2022-06-23T05:42:51.000872Z","iopub.status.idle":"2022-06-23T05:42:51.008730Z","shell.execute_reply.started":"2022-06-23T05:42:51.000843Z","shell.execute_reply":"2022-06-23T05:42:51.007552Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y) # stratify=y","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:42:51.010761Z","iopub.execute_input":"2022-06-23T05:42:51.011149Z","iopub.status.idle":"2022-06-23T05:42:51.055590Z","shell.execute_reply.started":"2022-06-23T05:42:51.011116Z","shell.execute_reply":"2022-06-23T05:42:51.054611Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer()\n\nX_train_review_bow = vect.fit_transform(X_train['text_Cleaned'])\nX_test_review_bow = vect.transform(X_test['text_Cleaned'])\nX_sub_rev_bow = vect.transform(test['text_Cleaned'])\n#X_train_review_bow = vect.fit_transform(X_train)\n#X_test_review_bow = vect.transform(X_test)\n\nprint('X_train_review_bow shape: ', X_train_review_bow.shape)\nprint('X_test_review_bow shape: ', X_test_review_bow.shape)\nprint('X_sub_review_bow shape: ', X_sub_rev_bow.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:42:51.056610Z","iopub.execute_input":"2022-06-23T05:42:51.057361Z","iopub.status.idle":"2022-06-23T05:42:53.785185Z","shell.execute_reply.started":"2022-06-23T05:42:51.057323Z","shell.execute_reply":"2022-06-23T05:42:53.784115Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Tf-Idf\nTf-Idf —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫ —á–∞—Å—Ç–æ—Ç–∞ —Ç–µ—Ä–º–∏–Ω–∞, –æ–±—Ä–∞—Ç–Ω–∞—è —á–∞—Å—Ç–æ—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –∏ –≤–º–µ—Å—Ç–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –∫–∞–∂–¥–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö (Bow) –æ–Ω –≤—ã—á–∏—Å–ª—è–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, –≥–¥–µ –∫–∞–∂–¥–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–µ–ª–∏—Ç—Å—è –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —ç—Ç–æ —Å–ª–æ–≤–æ. .\n\nTf-idf(w, d)= Bow(w, d) * log(–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤/(–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Å–ª–æ–≤–æ w))\n\n–ï—Å–ª–∏ —Å–ª–æ–≤–æ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ, –Ω–æ –Ω–µ –≤–æ –º–Ω–æ–≥–∏—Ö –¥—Ä—É–≥–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ, —á—Ç–æ —ç—Ç–æ —Å–ª–æ–≤–æ –∏–º–µ–µ—Ç –æ—Å–æ–±–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –ø–æ–ª—É—á–∞–µ—Ç –±–æ–ª—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, —á–µ–º —Ä–∞–Ω—å—à–µ, –±–ª–∞–≥–æ–¥–∞—Ä—è –≤—ã—Å–æ–∫–æ–º—É Idf. –° –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, –µ—Å–ª–∏ —Å–ª–æ–≤–æ –ø–æ—è–≤–ª—è–µ—Ç—Å—è –≤–æ –º–Ω–æ–≥–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, —Ç–æ –µ–≥–æ Idf –±–ª–∏–∑–æ–∫ –∫ 1, –∞ –ª–æ–≥–∞—Ä–∏—Ñ–º –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç 1 –≤ 0 –∏ —É–º–µ–Ω—å—à–∞–µ—Ç –µ–≥–æ –≤–ª–∏—è–Ω–∏–µ.","metadata":{}},{"cell_type":"code","source":"# –°–æ–∑–¥–∞–π—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ tf-idf, –∏—Å–ø–æ–ª—å–∑—É—è –º–∞—Ç—Ä–∏—Ü—É –Ω–∞–±–æ—Ä–∞ —Å–ª–æ–≤\n#tfidf_transform = text.TfidfTransformer(norm=None) \nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\n\nX_train_review_tfidf = vectorizer.fit_transform(X_train['text_Cleaned'])\nX_test_review_tfidf = vectorizer.transform(X_test['text_Cleaned'])\nX_sub_review_tfidf = vectorizer.transform(test['text_Cleaned'])\n\nprint('X_train_review_tfidf shape: ', X_train_review_tfidf.shape)\nprint('X_test_review_tfidf shape: ', X_test_review_tfidf.shape)\nprint('X_sub_review_tfidf shape: ', X_sub_review_tfidf.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:42:53.786344Z","iopub.execute_input":"2022-06-23T05:42:53.786675Z","iopub.status.idle":"2022-06-23T05:42:56.620095Z","shell.execute_reply.started":"2022-06-23T05:42:53.786644Z","shell.execute_reply":"2022-06-23T05:42:56.618990Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è\n–ü–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è 70/30 —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –ø–æ–µ–∑–¥-—Ç–µ—Å—Ç —è –ø—Ä–∏–º–µ–Ω–∏–ª –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–∑–≤–µ—à–µ–Ω–Ω—É—é –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –≤—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç –∏—Ö —á–µ—Ä–µ–∑ —Å–∏–≥–º–æ–≤–∏–¥–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é. –°–∏–≥–º–æ–≤–∏–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ª—é–±–æ–µ –≤–≤–µ–¥–µ–Ω–Ω–æ–µ –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ –≤ —á–∏—Å–ª–æ –æ—Ç 0 –¥–æ 1.","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\ny_test_arg=np.argmax(y_test)\nclf = MultinomialNB()\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow) #prediction from model\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Recall: ', recall_score(y_test, y_pred, average = 'micro'))\nprint('Precision: ', precision_score(y_test, y_pred, average = 'weighted', zero_division = 1))\n#print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:42:56.621996Z","iopub.execute_input":"2022-06-23T05:42:56.622339Z","iopub.status.idle":"2022-06-23T05:42:57.000358Z","shell.execute_reply.started":"2022-06-23T05:42:56.622309Z","shell.execute_reply":"2022-06-23T05:42:56.999598Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"clf = MultinomialNB()\ny_test_arg=np.argmax(y_test)\nclf.fit(X_train_review_tfidf, y_train)\n\ny_pred = clf.predict(X_test_review_tfidf)\n#y_pred = clf.predict(X_sub_review_tfidf)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:43:00.896065Z","iopub.execute_input":"2022-06-23T05:43:00.897248Z","iopub.status.idle":"2022-06-23T05:43:01.271745Z","shell.execute_reply.started":"2022-06-23T05:43:00.897189Z","shell.execute_reply":"2022-06-23T05:43:01.270942Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', random_state=42)\nclf.fit(X_train_review_tfidf, y_train)\n\n#y_pred = clf.predict(X_test_review_tfidf)\ny_pred = clf.predict(X_test_review_tfidf)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:43:02.235429Z","iopub.execute_input":"2022-06-23T05:43:02.236133Z","iopub.status.idle":"2022-06-23T05:45:13.419357Z","shell.execute_reply.started":"2022-06-23T05:43:02.236088Z","shell.execute_reply":"2022-06-23T05:45:13.418322Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:45:13.421030Z","iopub.execute_input":"2022-06-23T05:45:13.421415Z","iopub.status.idle":"2022-06-23T05:47:23.724656Z","shell.execute_reply.started":"2022-06-23T05:45:13.421383Z","shell.execute_reply":"2022-06-23T05:47:23.723706Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## SMOTE","metadata":{}},{"cell_type":"code","source":"from numpy import mean\nfrom numpy import std\nfrom pandas import read_csv\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\n \n\n# evaluate a model\ndef evaluate_model(X, y, model):\n# define evaluation procedure\n    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n# evaluate model\n    scores = cross_val_score(model, X_train_review_tfidf, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n \n# load the dataset\n#X, y = load_dataset(full_path)\n# define the model\nmodel = RandomForestClassifier(n_estimators=1000, class_weight='balanced')\n# evaluate the model\nscores = evaluate_model(X_train_review_tfidf, y_train, model)\n# summarize performance\nprint('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))","metadata":{"execution":{"iopub.status.busy":"2022-06-22T20:18:55.258109Z","iopub.execute_input":"2022-06-22T20:18:55.258546Z","iopub.status.idle":"2022-06-22T21:10:51.851244Z","shell.execute_reply.started":"2022-06-22T20:18:55.258511Z","shell.execute_reply":"2022-06-22T21:10:51.848059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ngramm","metadata":{}},{"cell_type":"code","source":"# CountVectorizer\nvect = CountVectorizer(ngram_range=(1, 2))\n\nX_train_review_bow = vect.fit_transform(X_train['text_Cleaned'])\nX_test_review_bow = vect.transform(X_test['text_Cleaned'])\nX_sub_rev_bow = vect.transform(test['text_Cleaned'])","metadata":{"execution":{"iopub.status.busy":"2022-06-19T08:38:22.279002Z","iopub.execute_input":"2022-06-19T08:38:22.279513Z","iopub.status.idle":"2022-06-19T08:38:36.795565Z","shell.execute_reply.started":"2022-06-19T08:38:22.279475Z","shell.execute_reply":"2022-06-19T08:38:36.79414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C = 10, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T15:47:17.140572Z","iopub.execute_input":"2022-06-20T15:47:17.141507Z","iopub.status.idle":"2022-06-20T15:49:21.515959Z","shell.execute_reply.started":"2022-06-20T15:47:17.141459Z","shell.execute_reply":"2022-06-20T15:49:21.515179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='newton-cg', penalty='l2', C = 10, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:09:00.408055Z","iopub.execute_input":"2022-06-20T16:09:00.408626Z","iopub.status.idle":"2022-06-20T16:13:28.704309Z","shell.execute_reply.started":"2022-06-20T16:09:00.408582Z","shell.execute_reply":"2022-06-20T16:13:28.703168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='saga', penalty='l2', C = 100, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:01:37.696861Z","iopub.execute_input":"2022-06-20T16:01:37.697771Z","iopub.status.idle":"2022-06-20T16:02:23.440693Z","shell.execute_reply.started":"2022-06-20T16:01:37.697715Z","shell.execute_reply":"2022-06-20T16:02:23.439403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C = 10, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:05:23.075671Z","iopub.execute_input":"2022-06-20T16:05:23.076136Z","iopub.status.idle":"2022-06-20T16:07:28.599576Z","shell.execute_reply.started":"2022-06-20T16:05:23.076101Z","shell.execute_reply":"2022-06-20T16:07:28.598573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HashingVectorizer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import HashingVectorizer\nvectorizer = HashingVectorizer(ngram_range=(1, 4))\n\nX_train_review_hv = vectorizer.fit_transform(X_train['text_Cleaned'])\nX_test_review_hv = vectorizer.transform(X_test['text_Cleaned'])\nX_sub_review_hv = vectorizer.transform(test['text_Cleaned'])","metadata":{"execution":{"iopub.status.busy":"2022-06-19T08:22:03.390546Z","iopub.execute_input":"2022-06-19T08:22:03.391076Z","iopub.status.idle":"2022-06-19T08:22:09.618556Z","shell.execute_reply.started":"2022-06-19T08:22:03.391037Z","shell.execute_reply":"2022-06-19T08:22:09.617247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', random_state=42)\nclf.fit(X_train_review_hv, y_train)\n\ny_pred = clf.predict(X_test_review_hv)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T12:58:42.892913Z","iopub.execute_input":"2022-06-19T12:58:42.893329Z","iopub.status.idle":"2022-06-19T12:58:42.921097Z","shell.execute_reply.started":"2022-06-19T12:58:42.893299Z","shell.execute_reply":"2022-06-19T12:58:42.919644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVC","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train_review_bow, y_train)\ny_pred_svc = svclassifier.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred_svc))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:53:31.038874Z","iopub.execute_input":"2022-06-23T05:53:31.040597Z","iopub.status.idle":"2022-06-23T06:06:57.554568Z","shell.execute_reply.started":"2022-06-23T05:53:31.040549Z","shell.execute_reply":"2022-06-23T06:06:57.553349Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## KNN","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 10)\nknn.fit(X_train_review_bow, y_train)\ny_pred = knn.predict(X_test_review_bow)\n\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:16:45.639192Z","iopub.execute_input":"2022-06-17T16:16:45.639944Z","iopub.status.idle":"2022-06-17T16:17:05.636452Z","shell.execute_reply.started":"2022-06-17T16:16:45.639905Z","shell.execute_reply":"2022-06-17T16:17:05.635328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## –¥–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ Test –∏ –≤—ã–∫–ª–∞–¥—ã–≤–∞–µ–º –Ω–∞ Kaggle","metadata":{}},{"cell_type":"code","source":"clf = MultinomialNB(alpha=1)\nclf.fit(X_train_review_tfidf, y_train)\n\ny_pred = clf.predict(X_sub_review_tfidf)\n#print('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-17T10:38:30.797143Z","iopub.execute_input":"2022-06-17T10:38:30.797844Z","iopub.status.idle":"2022-06-17T10:38:30.841422Z","shell.execute_reply.started":"2022-06-17T10:38:30.797803Z","shell.execute_reply":"2022-06-17T10:38:30.840259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train_review_bow, y_train)\ny_pred_svc = svclassifier.predict(X_sub_review_tfidf)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:46:02.498956Z","iopub.execute_input":"2022-06-17T15:46:02.499398Z","iopub.status.idle":"2022-06-17T16:01:53.196087Z","shell.execute_reply.started":"2022-06-17T15:46:02.49935Z","shell.execute_reply":"2022-06-17T16:01:53.195358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(random_state=42)\nclf.fit(X_train_review_tfidf, y_train)\n\n#y_pred = clf.predict(X_test_review_tfidf)\ny_pred = clf.predict(X_sub_review_tfidf)\n#print('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-16T10:23:37.31609Z","iopub.execute_input":"2022-06-16T10:23:37.317114Z","iopub.status.idle":"2022-06-16T10:24:17.197726Z","shell.execute_reply.started":"2022-06-16T10:23:37.317069Z","shell.execute_reply":"2022-06-16T10:24:17.19669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(multi_class='multinomial', solver='saga', penalty='l2', C = 100, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_sub_rev_bow)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:03:50.058974Z","iopub.execute_input":"2022-06-20T16:03:50.059506Z","iopub.status.idle":"2022-06-20T16:04:35.349235Z","shell.execute_reply.started":"2022-06-20T16:03:50.059463Z","shell.execute_reply":"2022-06-20T16:04:35.348159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(train['class'].unique())\n#list(le.classes_)\nnum = le.transform(train['class'].unique())\n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:04:35.35156Z","iopub.execute_input":"2022-06-20T16:04:35.352292Z","iopub.status.idle":"2022-06-20T16:04:35.370807Z","shell.execute_reply.started":"2022-06-20T16:04:35.352245Z","shell.execute_reply":"2022-06-20T16:04:35.369968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_sub = list(le.inverse_transform(y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:04:35.372062Z","iopub.execute_input":"2022-06-20T16:04:35.372555Z","iopub.status.idle":"2022-06-20T16:04:35.381029Z","shell.execute_reply.started":"2022-06-20T16:04:35.372511Z","shell.execute_reply":"2022-06-20T16:04:35.379896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sub_predict = model.predict(X_sub)\nsample_submission['class'] = y_sub\nsample_submission.to_csv('LR_submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:04:35.382987Z","iopub.execute_input":"2022-06-20T16:04:35.383641Z","iopub.status.idle":"2022-06-20T16:04:35.452187Z","shell.execute_reply.started":"2022-06-20T16:04:35.383596Z","shell.execute_reply":"2022-06-20T16:04:35.451484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"–Ø –ø—Ä–∏–º–µ–Ω–∏–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∫ —Ñ—É–Ω–∫—Ü–∏—è–º Bag-of-triGrams –∏ Tf-Idf, —á—Ç–æ–±—ã —Å—Ä–∞–≤–Ω–∏—Ç—å –∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–∞–µ—Ç –Ω–∞–º –æ—Ü–µ–Ω–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏, –∫–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –Ω–∏–∂–µ:","metadata":{}},{"cell_type":"markdown","source":"## –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(multi_class='multinomial', penalty='l2', random_state=42)\n#penalty = ['l1','l2']\nsolver = ['newton-cg', 'sag', 'saga', 'lbfgs']\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\nalpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\ngrid = dict(alpha=alpha)\nc_values = [100, 10, 1.0]\n#param_grid = {'C':[1, 10]}\nparam_grid = dict(C=c_values, solver=solver)\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, verbose=10, cv=cv, scoring='accuracy', error_score=0)\ngrid_search.fit(X_train_review_bow, y_train)\n#print(\"Best: %f using %s\" % (grid_search.best_params_))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T19:18:27.586886Z","iopub.execute_input":"2022-06-19T19:18:27.587344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#grid_search.fit(X_train_review_bow, y_train)\nprint(\"Best: %f using %s\" % (grid_search.best_params_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## –° –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ–ª—É—á–∏–ª–æ—Å—å —É–ª—É—á—à–∏—Ç—å –º–æ–¥–µ–ª—å –¥–æ 0.897","metadata":{}},{"cell_type":"markdown","source":"## Fine Tuning DistilBERT TensorFlow","metadata":{}},{"cell_type":"code","source":"!pip uninstall transformers\n!pip uninstall h5py\n\n!pip install transformers==3.1\n!pip install h5py==2.10.0","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:23:31.521709Z","iopub.execute_input":"2022-06-23T06:23:31.522172Z","iopub.status.idle":"2022-06-23T06:26:29.470715Z","shell.execute_reply.started":"2022-06-23T06:23:31.522136Z","shell.execute_reply":"2022-06-23T06:26:29.469396Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"print(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:26:59.805294Z","iopub.execute_input":"2022-06-23T06:26:59.805704Z","iopub.status.idle":"2022-06-23T06:26:59.810981Z","shell.execute_reply.started":"2022-06-23T06:26:59.805671Z","shell.execute_reply":"2022-06-23T06:26:59.809746Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import DistilBertTokenizer\nfrom transformers import TFDistilBertForSequenceClassification\nimport tensorflow as tf\nimport pandas as pd\nimport json\nimport gc\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:13:10.496854Z","iopub.execute_input":"2022-06-23T06:13:10.497335Z","iopub.status.idle":"2022-06-23T06:13:10.630454Z","shell.execute_reply.started":"2022-06-23T06:13:10.497295Z","shell.execute_reply":"2022-06-23T06:13:10.628505Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True)\n#val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:13:23.488345Z","iopub.execute_input":"2022-06-23T06:13:23.489195Z","iopub.status.idle":"2022-06-23T06:13:23.526167Z","shell.execute_reply.started":"2022-06-23T06:13:23.489148Z","shell.execute_reply":"2022-06-23T06:13:23.524706Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings),\n    train_labels\n))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), \n                                    list(test_labels))) \n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(val_encodings),\n    val_labels\n))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=50)\nlosss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) # 2e-5\nmodel.compile(optimizer=optimizer, loss=losss, metrics=['accuracy']) # loss=model.compute_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_dataset.shuffle(1000).batch(16), \n          epochs=10)\n          validation_data=val_dataset.shuffle(1000).batch(16))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-tuning Option 2: Using the TFTrainer class","metadata":{}},{"cell_type":"code","source":"from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n\ntraining_args = TFTrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=3,              # total number of training epochs\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=64,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    #logging_dir='./logs',            # directory for storing logs\n)\n\nwith training_args.strategy.scope():\n    trainer_model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=50)\n\ntrainer = TFTrainer(\n    model=trainer_model,                 # the instantiated ü§ó Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset,             # evaluation dataset\n)","metadata":{},"execution_count":null,"outputs":[]}]}
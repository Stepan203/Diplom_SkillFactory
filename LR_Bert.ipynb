{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SCAN (https://scan-interfax.ru/) - это система Международной информационной группы \"Интерфакс\", предназначенная для комплексного решения задач в сфере управления репутацией и анализа эффективности PR. Решает задачи по мониторингу СМИ и соцсетей, анализу медиа-поля, проверке деловой репутации компаний и персон, позволяет пользователям оперативно реагировать на появление негатива в СМИ и соцмедиа, путём осуществления автоматизированного сбора и анализа публичной информации из более чем 60 тысяч источников.\n\nОдной из функциональных возможностей SCAN является фактографический анализ новостной информации - выделение контекстов, в которых упоминается событие или действие некоторого субъекта на заданную тематику и с заданной тональностью.\nНа текущий момент SCAN умеет определять контексты более чем по 800 различным темам. Разработка каждой темы представляет собой длительный процесс по предварительному сбору и анализу большого корпуса текстовой информации для выделения характерных фраз, с последующим написанием машинных правил на специальном DSL (domain-specific language), в котором задействован целый отдел прикладных лингвистов.\n\nМы предлагаем вам принять участие в решении значимой для проекта SCAN задачи, которая позволяет расширить спектр выделяемых системой контекстов и снизит нагрузку на лингвистов:\n\nРазработка средства автоматизированного поиска контекстов на заданные тематики. Нам важна семантическая близость к уже проработанным нами контекстам, чтобы не требовалось описывать каждый контекст в рамках одной тематики машинными правилами на специальном DSL:\n\nВ качестве исходных данных выступают наборы размеченных корпусов на различные тематики.\nВ качестве искомого контекста может выступать как часть предложения исходного текста, так и целое предложение, или даже набор предложений на ту же тему.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstopwords_ru = stopwords.words(\"russian\")\nfrom nltk.stem import WordNetLemmatizer\n\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\nimport transformers\nimport torch\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer, BertModel, BertConfig","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:39:37.355579Z","iopub.execute_input":"2022-06-23T05:39:37.356364Z","iopub.status.idle":"2022-06-23T05:39:48.007409Z","shell.execute_reply.started":"2022-06-23T05:39:37.356219Z","shell.execute_reply":"2022-06-23T05:39:48.006308Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"## Применение классификации текста с использованием логистической регрессии","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/scan-classification-challange/df_train.csv')\ntrain.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:39:48.009145Z","iopub.execute_input":"2022-06-23T05:39:48.009761Z","iopub.status.idle":"2022-06-23T05:39:48.685983Z","shell.execute_reply.started":"2022-06-23T05:39:48.009725Z","shell.execute_reply":"2022-06-23T05:39:48.684760Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/scan-classification-challange/df_test.csv', index_col=0)\ntest.sample(3)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:39:48.687458Z","iopub.execute_input":"2022-06-23T05:39:48.687882Z","iopub.status.idle":"2022-06-23T05:39:49.086464Z","shell.execute_reply.started":"2022-06-23T05:39:48.687847Z","shell.execute_reply":"2022-06-23T05:39:49.085483Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Удаляем дубликаты\ntrain.drop_duplicates(subset={'text'}, inplace=True) \ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:39:49.088703Z","iopub.execute_input":"2022-06-23T05:39:49.089325Z","iopub.status.idle":"2022-06-23T05:39:49.215792Z","shell.execute_reply.started":"2022-06-23T05:39:49.089254Z","shell.execute_reply":"2022-06-23T05:39:49.214692Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/scan-classification-challange/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:39:49.216959Z","iopub.execute_input":"2022-06-23T05:39:49.217263Z","iopub.status.idle":"2022-06-23T05:39:49.246189Z","shell.execute_reply.started":"2022-06-23T05:39:49.217236Z","shell.execute_reply":"2022-06-23T05:39:49.245207Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train['encoded_cat'] = train['class'].astype('category').cat.codes\n#train.drop(['class'], inplace=True, axis=1)\ntrain.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:39:49.247597Z","iopub.execute_input":"2022-06-23T05:39:49.247908Z","iopub.status.idle":"2022-06-23T05:39:49.269058Z","shell.execute_reply.started":"2022-06-23T05:39:49.247878Z","shell.execute_reply":"2022-06-23T05:39:49.268104Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#train['sample'] = 1 # помечаем где у нас трейн\n#test['sample'] = 0 # помечаем где у нас тест\n#test['class'] = 0 # в тесте у нас нет значения price, мы его должны предсказать, поэтому пока просто заполняем нулями\n\n#df = test.append(train, sort=False).reset_index(drop=True) # объединяем\n#print(train.shape, test.shape, df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T06:42:04.427314Z","iopub.execute_input":"2022-06-17T06:42:04.428061Z","iopub.status.idle":"2022-06-17T06:42:04.432238Z","shell.execute_reply.started":"2022-06-17T06:42:04.428021Z","shell.execute_reply":"2022-06-17T06:42:04.431434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    '''Text Preprocessing '''\n    \n    # Convert words to lower case\n    text = text.lower()\n    contractions = []\n    #Expand contractions\n    if True:\n        text = text.split()\n        new_text = []\n        for word in text:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n        text = \" \".join(new_text)\n    \n    # Format words and remove unwanted characters\n    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\<a href', ' ', text)\n    text = re.sub(r'&amp;', '', text) \n    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/0-9]', ' ', text)\n    #text = re.sub('[^A-Za-z0-9]+', ' ', text)\n    text = re.sub(r'<br />', ' ', text)\n    text = re.sub(r'\\'', ' ', text)\n    \n    # remove stopwords\n   # if remove_stopwords:\n    text = text.split()\n    stops = set(stopwords.words(\"russian\"))\n    text = [w for w in text if not w in stops]\n   # text = \" \".join(text)\n\n    # Tokenize each word\n    #text =  nltk.WordPunctTokenizer().tokenize(text)\n    \n    # Lemmatize each token\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    text = [lemmatizer.lemmatize(word) for word in text]\n   # lemm = nltk.stem.WordNetLemmatizer()\n    #text = list(map(lambda word:list(map(lemm.lemmatize, word)), text))\n    text = \" \".join(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:41:49.670451Z","iopub.execute_input":"2022-06-23T05:41:49.671448Z","iopub.status.idle":"2022-06-23T05:41:49.687218Z","shell.execute_reply.started":"2022-06-23T05:41:49.671258Z","shell.execute_reply":"2022-06-23T05:41:49.685875Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\"\"\"def clean_word(text):\n    '''Text Preprocessing '''\n    \n    # Convert words to lower case\n    text = text.lower()\n    contractions = []\n    #Expand contractions\n    if True:\n        text = text.split()\n        new_text = []\n        for word in text:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n        text = \" \".join(new_text)\n    \n    # Format words and remove unwanted characters\n    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\<a href', ' ', text)\n    text = re.sub(r'&amp;', '', text) \n    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/0-9]', ' ', text)\n    #text = re.sub('[^A-Za-z0-9]+', ' ', text)\n    text = re.sub(r'<br />', ' ', text)\n    text = re.sub(r'\\'', ' ', text)\n    \n    # remove stopwords\n   # if remove_stopwords:\n    text = text.split()\n    stops = set(stopwords.words(\"russian\"))\n    text = [w for w in text if not w in stops]\n   # text = \" \".join(text)\n\n    # Tokenize each word\n    #text =  nltk.WordPunctTokenizer().tokenize(text)\n    \n    # Lemmatize each token\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    text = [lemmatizer.lemmatize(word) for word in text]\n   # lemm = nltk.stem.WordNetLemmatizer()\n    #text = list(map(lambda word:list(map(lemm.lemmatize, word)), text))\n    text = \" \".join(text)\n    text = list(text)\n    text = \" \".join(text)\n    return text\n    \"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:41:06.193077Z","iopub.execute_input":"2022-06-23T05:41:06.193994Z","iopub.status.idle":"2022-06-23T05:41:06.206735Z","shell.execute_reply.started":"2022-06-23T05:41:06.193937Z","shell.execute_reply":"2022-06-23T05:41:06.205423Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#train['text_word'] = train['text'].apply(lambda x: clean_word(x))\n#train[['text_word', 'text']]","metadata":{"execution":{"iopub.status.busy":"2022-06-22T20:16:56.41072Z","iopub.execute_input":"2022-06-22T20:16:56.411146Z","iopub.status.idle":"2022-06-22T20:17:12.982688Z","shell.execute_reply.started":"2022-06-22T20:16:56.411112Z","shell.execute_reply":"2022-06-22T20:17:12.981639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text_Cleaned'] = train['text'].apply(lambda x: clean_text(x))\ntrain[['text_Cleaned', 'text']]\n#train['text_Cleaned'] = train['text'].apply(lambda x: clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:42:22.709175Z","iopub.execute_input":"2022-06-23T05:42:22.709666Z","iopub.status.idle":"2022-06-23T05:42:40.675779Z","shell.execute_reply.started":"2022-06-23T05:42:22.709627Z","shell.execute_reply":"2022-06-23T05:42:40.674781Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"test['text_Cleaned'] = test['text'].apply(lambda x: clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:42:40.677724Z","iopub.execute_input":"2022-06-23T05:42:40.678264Z","iopub.status.idle":"2022-06-23T05:42:50.999064Z","shell.execute_reply.started":"2022-06-23T05:42:40.678215Z","shell.execute_reply":"2022-06-23T05:42:50.997976Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Splitting data(80-20): Train | Test","metadata":{}},{"cell_type":"code","source":"y = train.encoded_cat.values     # наш таргет\nX = train.drop(['encoded_cat', 'text'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:42:51.000543Z","iopub.execute_input":"2022-06-23T05:42:51.000872Z","iopub.status.idle":"2022-06-23T05:42:51.008730Z","shell.execute_reply.started":"2022-06-23T05:42:51.000843Z","shell.execute_reply":"2022-06-23T05:42:51.007552Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y) # stratify=y","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:42:51.010761Z","iopub.execute_input":"2022-06-23T05:42:51.011149Z","iopub.status.idle":"2022-06-23T05:42:51.055590Z","shell.execute_reply.started":"2022-06-23T05:42:51.011116Z","shell.execute_reply":"2022-06-23T05:42:51.054611Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer()\n\nX_train_review_bow = vect.fit_transform(X_train['text_Cleaned'])\nX_test_review_bow = vect.transform(X_test['text_Cleaned'])\nX_sub_rev_bow = vect.transform(test['text_Cleaned'])\n#X_train_review_bow = vect.fit_transform(X_train)\n#X_test_review_bow = vect.transform(X_test)\n\nprint('X_train_review_bow shape: ', X_train_review_bow.shape)\nprint('X_test_review_bow shape: ', X_test_review_bow.shape)\nprint('X_sub_review_bow shape: ', X_sub_rev_bow.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:42:51.056610Z","iopub.execute_input":"2022-06-23T05:42:51.057361Z","iopub.status.idle":"2022-06-23T05:42:53.785185Z","shell.execute_reply.started":"2022-06-23T05:42:51.057323Z","shell.execute_reply":"2022-06-23T05:42:53.784115Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Tf-Idf\nTf-Idf расшифровывается как частота термина, обратная частоте документа, и вместо вычисления количества каждого слова в каждом документе набора данных (Bow) он вычисляет нормализованное количество, где каждое количество слов делится на количество документов, в которых встречается это слово. .\n\nTf-idf(w, d)= Bow(w, d) * log(Общее количество документов/(Количество документов, в которых встречается слово w))\n\nЕсли слово часто встречается в определенном документе, но не во многих других документах, наиболее вероятно, что это слово имеет особое значение для этого документа и получает большее количество, чем раньше, благодаря высокому Idf. С другой стороны, если слово появляется во многих документах, то его Idf близок к 1, а логарифм превращает 1 в 0 и уменьшает его влияние.","metadata":{}},{"cell_type":"code","source":"# Создайте представление tf-idf, используя матрицу набора слов\n#tfidf_transform = text.TfidfTransformer(norm=None) \nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\n\nX_train_review_tfidf = vectorizer.fit_transform(X_train['text_Cleaned'])\nX_test_review_tfidf = vectorizer.transform(X_test['text_Cleaned'])\nX_sub_review_tfidf = vectorizer.transform(test['text_Cleaned'])\n\nprint('X_train_review_tfidf shape: ', X_train_review_tfidf.shape)\nprint('X_test_review_tfidf shape: ', X_test_review_tfidf.shape)\nprint('X_sub_review_tfidf shape: ', X_sub_review_tfidf.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:42:53.786344Z","iopub.execute_input":"2022-06-23T05:42:53.786675Z","iopub.status.idle":"2022-06-23T05:42:56.620095Z","shell.execute_reply.started":"2022-06-23T05:42:53.786644Z","shell.execute_reply":"2022-06-23T05:42:56.618990Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Логистическая регрессия\nПосле создания 70/30 разделения набора данных на поезд-тест я применил логистическую регрессию, которая представляет собой алгоритм классификации, используемый для решения задач бинарной классификации. Классификатор логистической регрессии использует взвешенную комбинацию входных признаков и пропускает их через сигмовидную функцию. Сигмовидная функция преобразует любое введенное вещественное число в число от 0 до 1.","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\ny_test_arg=np.argmax(y_test)\nclf = MultinomialNB()\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow) #prediction from model\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Recall: ', recall_score(y_test, y_pred, average = 'micro'))\nprint('Precision: ', precision_score(y_test, y_pred, average = 'weighted', zero_division = 1))\n#print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:42:56.621996Z","iopub.execute_input":"2022-06-23T05:42:56.622339Z","iopub.status.idle":"2022-06-23T05:42:57.000358Z","shell.execute_reply.started":"2022-06-23T05:42:56.622309Z","shell.execute_reply":"2022-06-23T05:42:56.999598Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"clf = MultinomialNB()\ny_test_arg=np.argmax(y_test)\nclf.fit(X_train_review_tfidf, y_train)\n\ny_pred = clf.predict(X_test_review_tfidf)\n#y_pred = clf.predict(X_sub_review_tfidf)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:43:00.896065Z","iopub.execute_input":"2022-06-23T05:43:00.897248Z","iopub.status.idle":"2022-06-23T05:43:01.271745Z","shell.execute_reply.started":"2022-06-23T05:43:00.897189Z","shell.execute_reply":"2022-06-23T05:43:01.270942Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', random_state=42)\nclf.fit(X_train_review_tfidf, y_train)\n\n#y_pred = clf.predict(X_test_review_tfidf)\ny_pred = clf.predict(X_test_review_tfidf)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:43:02.235429Z","iopub.execute_input":"2022-06-23T05:43:02.236133Z","iopub.status.idle":"2022-06-23T05:45:13.419357Z","shell.execute_reply.started":"2022-06-23T05:43:02.236088Z","shell.execute_reply":"2022-06-23T05:45:13.418322Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:45:13.421030Z","iopub.execute_input":"2022-06-23T05:45:13.421415Z","iopub.status.idle":"2022-06-23T05:47:23.724656Z","shell.execute_reply.started":"2022-06-23T05:45:13.421383Z","shell.execute_reply":"2022-06-23T05:47:23.723706Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## SMOTE","metadata":{}},{"cell_type":"code","source":"from numpy import mean\nfrom numpy import std\nfrom pandas import read_csv\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\n \n\n# evaluate a model\ndef evaluate_model(X, y, model):\n# define evaluation procedure\n    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n# evaluate model\n    scores = cross_val_score(model, X_train_review_tfidf, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n \n# load the dataset\n#X, y = load_dataset(full_path)\n# define the model\nmodel = RandomForestClassifier(n_estimators=1000, class_weight='balanced')\n# evaluate the model\nscores = evaluate_model(X_train_review_tfidf, y_train, model)\n# summarize performance\nprint('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))","metadata":{"execution":{"iopub.status.busy":"2022-06-22T20:18:55.258109Z","iopub.execute_input":"2022-06-22T20:18:55.258546Z","iopub.status.idle":"2022-06-22T21:10:51.851244Z","shell.execute_reply.started":"2022-06-22T20:18:55.258511Z","shell.execute_reply":"2022-06-22T21:10:51.848059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ngramm","metadata":{}},{"cell_type":"code","source":"# CountVectorizer\nvect = CountVectorizer(ngram_range=(1, 2))\n\nX_train_review_bow = vect.fit_transform(X_train['text_Cleaned'])\nX_test_review_bow = vect.transform(X_test['text_Cleaned'])\nX_sub_rev_bow = vect.transform(test['text_Cleaned'])","metadata":{"execution":{"iopub.status.busy":"2022-06-19T08:38:22.279002Z","iopub.execute_input":"2022-06-19T08:38:22.279513Z","iopub.status.idle":"2022-06-19T08:38:36.795565Z","shell.execute_reply.started":"2022-06-19T08:38:22.279475Z","shell.execute_reply":"2022-06-19T08:38:36.79414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C = 10, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T15:47:17.140572Z","iopub.execute_input":"2022-06-20T15:47:17.141507Z","iopub.status.idle":"2022-06-20T15:49:21.515959Z","shell.execute_reply.started":"2022-06-20T15:47:17.141459Z","shell.execute_reply":"2022-06-20T15:49:21.515179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='newton-cg', penalty='l2', C = 10, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:09:00.408055Z","iopub.execute_input":"2022-06-20T16:09:00.408626Z","iopub.status.idle":"2022-06-20T16:13:28.704309Z","shell.execute_reply.started":"2022-06-20T16:09:00.408582Z","shell.execute_reply":"2022-06-20T16:13:28.703168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='saga', penalty='l2', C = 100, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:01:37.696861Z","iopub.execute_input":"2022-06-20T16:01:37.697771Z","iopub.status.idle":"2022-06-20T16:02:23.440693Z","shell.execute_reply.started":"2022-06-20T16:01:37.697715Z","shell.execute_reply":"2022-06-20T16:02:23.439403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C = 10, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:05:23.075671Z","iopub.execute_input":"2022-06-20T16:05:23.076136Z","iopub.status.idle":"2022-06-20T16:07:28.599576Z","shell.execute_reply.started":"2022-06-20T16:05:23.076101Z","shell.execute_reply":"2022-06-20T16:07:28.598573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HashingVectorizer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import HashingVectorizer\nvectorizer = HashingVectorizer(ngram_range=(1, 4))\n\nX_train_review_hv = vectorizer.fit_transform(X_train['text_Cleaned'])\nX_test_review_hv = vectorizer.transform(X_test['text_Cleaned'])\nX_sub_review_hv = vectorizer.transform(test['text_Cleaned'])","metadata":{"execution":{"iopub.status.busy":"2022-06-19T08:22:03.390546Z","iopub.execute_input":"2022-06-19T08:22:03.391076Z","iopub.status.idle":"2022-06-19T08:22:09.618556Z","shell.execute_reply.started":"2022-06-19T08:22:03.391037Z","shell.execute_reply":"2022-06-19T08:22:09.617247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', random_state=42)\nclf.fit(X_train_review_hv, y_train)\n\ny_pred = clf.predict(X_test_review_hv)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T12:58:42.892913Z","iopub.execute_input":"2022-06-19T12:58:42.893329Z","iopub.status.idle":"2022-06-19T12:58:42.921097Z","shell.execute_reply.started":"2022-06-19T12:58:42.893299Z","shell.execute_reply":"2022-06-19T12:58:42.919644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVC","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train_review_bow, y_train)\ny_pred_svc = svclassifier.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred_svc))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T05:53:31.038874Z","iopub.execute_input":"2022-06-23T05:53:31.040597Z","iopub.status.idle":"2022-06-23T06:06:57.554568Z","shell.execute_reply.started":"2022-06-23T05:53:31.040549Z","shell.execute_reply":"2022-06-23T06:06:57.553349Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## KNN","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 10)\nknn.fit(X_train_review_bow, y_train)\ny_pred = knn.predict(X_test_review_bow)\n\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:16:45.639192Z","iopub.execute_input":"2022-06-17T16:16:45.639944Z","iopub.status.idle":"2022-06-17T16:17:05.636452Z","shell.execute_reply.started":"2022-06-17T16:16:45.639905Z","shell.execute_reply":"2022-06-17T16:17:05.635328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## делаем предсказание на Test и выкладываем на Kaggle","metadata":{}},{"cell_type":"code","source":"clf = MultinomialNB(alpha=1)\nclf.fit(X_train_review_tfidf, y_train)\n\ny_pred = clf.predict(X_sub_review_tfidf)\n#print('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-17T10:38:30.797143Z","iopub.execute_input":"2022-06-17T10:38:30.797844Z","iopub.status.idle":"2022-06-17T10:38:30.841422Z","shell.execute_reply.started":"2022-06-17T10:38:30.797803Z","shell.execute_reply":"2022-06-17T10:38:30.840259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train_review_bow, y_train)\ny_pred_svc = svclassifier.predict(X_sub_review_tfidf)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:46:02.498956Z","iopub.execute_input":"2022-06-17T15:46:02.499398Z","iopub.status.idle":"2022-06-17T16:01:53.196087Z","shell.execute_reply.started":"2022-06-17T15:46:02.49935Z","shell.execute_reply":"2022-06-17T16:01:53.195358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(random_state=42)\nclf.fit(X_train_review_tfidf, y_train)\n\n#y_pred = clf.predict(X_test_review_tfidf)\ny_pred = clf.predict(X_sub_review_tfidf)\n#print('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-16T10:23:37.31609Z","iopub.execute_input":"2022-06-16T10:23:37.317114Z","iopub.status.idle":"2022-06-16T10:24:17.197726Z","shell.execute_reply.started":"2022-06-16T10:23:37.317069Z","shell.execute_reply":"2022-06-16T10:24:17.19669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(multi_class='multinomial', solver='saga', penalty='l2', C = 100, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_sub_rev_bow)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:03:50.058974Z","iopub.execute_input":"2022-06-20T16:03:50.059506Z","iopub.status.idle":"2022-06-20T16:04:35.349235Z","shell.execute_reply.started":"2022-06-20T16:03:50.059463Z","shell.execute_reply":"2022-06-20T16:04:35.348159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(train['class'].unique())\n#list(le.classes_)\nnum = le.transform(train['class'].unique())\n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:04:35.35156Z","iopub.execute_input":"2022-06-20T16:04:35.352292Z","iopub.status.idle":"2022-06-20T16:04:35.370807Z","shell.execute_reply.started":"2022-06-20T16:04:35.352245Z","shell.execute_reply":"2022-06-20T16:04:35.369968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_sub = list(le.inverse_transform(y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:04:35.372062Z","iopub.execute_input":"2022-06-20T16:04:35.372555Z","iopub.status.idle":"2022-06-20T16:04:35.381029Z","shell.execute_reply.started":"2022-06-20T16:04:35.372511Z","shell.execute_reply":"2022-06-20T16:04:35.379896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sub_predict = model.predict(X_sub)\nsample_submission['class'] = y_sub\nsample_submission.to_csv('LR_submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:04:35.382987Z","iopub.execute_input":"2022-06-20T16:04:35.383641Z","iopub.status.idle":"2022-06-20T16:04:35.452187Z","shell.execute_reply.started":"2022-06-20T16:04:35.383596Z","shell.execute_reply":"2022-06-20T16:04:35.451484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Я применил классификатор логистической регрессии к функциям Bag-of-triGrams и Tf-Idf, чтобы сравнить их показатели точности. Построение моделей с параметрами по умолчанию дает нам оценки точности, как показано ниже:","metadata":{}},{"cell_type":"markdown","source":"## Гиперпараметры","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(multi_class='multinomial', penalty='l2', random_state=42)\n#penalty = ['l1','l2']\nsolver = ['newton-cg', 'sag', 'saga', 'lbfgs']\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\nalpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\ngrid = dict(alpha=alpha)\nc_values = [100, 10, 1.0]\n#param_grid = {'C':[1, 10]}\nparam_grid = dict(C=c_values, solver=solver)\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, verbose=10, cv=cv, scoring='accuracy', error_score=0)\ngrid_search.fit(X_train_review_bow, y_train)\n#print(\"Best: %f using %s\" % (grid_search.best_params_))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T19:18:27.586886Z","iopub.execute_input":"2022-06-19T19:18:27.587344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#grid_search.fit(X_train_review_bow, y_train)\nprint(\"Best: %f using %s\" % (grid_search.best_params_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## С подобранными гиперпараметрами получилось улучшить модель до 0.897","metadata":{}},{"cell_type":"markdown","source":"## Fine Tuning DistilBERT TensorFlow","metadata":{}},{"cell_type":"code","source":"!pip uninstall transformers\n!pip uninstall h5py\n\n!pip install transformers==3.1\n!pip install h5py==2.10.0","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:23:31.521709Z","iopub.execute_input":"2022-06-23T06:23:31.522172Z","iopub.status.idle":"2022-06-23T06:26:29.470715Z","shell.execute_reply.started":"2022-06-23T06:23:31.522136Z","shell.execute_reply":"2022-06-23T06:26:29.469396Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"print(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:26:59.805294Z","iopub.execute_input":"2022-06-23T06:26:59.805704Z","iopub.status.idle":"2022-06-23T06:26:59.810981Z","shell.execute_reply.started":"2022-06-23T06:26:59.805671Z","shell.execute_reply":"2022-06-23T06:26:59.809746Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import DistilBertTokenizer\nfrom transformers import TFDistilBertForSequenceClassification\nimport tensorflow as tf\nimport pandas as pd\nimport json\nimport gc\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:13:10.496854Z","iopub.execute_input":"2022-06-23T06:13:10.497335Z","iopub.status.idle":"2022-06-23T06:13:10.630454Z","shell.execute_reply.started":"2022-06-23T06:13:10.497295Z","shell.execute_reply":"2022-06-23T06:13:10.628505Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True)\n#val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T06:13:23.488345Z","iopub.execute_input":"2022-06-23T06:13:23.489195Z","iopub.status.idle":"2022-06-23T06:13:23.526167Z","shell.execute_reply.started":"2022-06-23T06:13:23.489148Z","shell.execute_reply":"2022-06-23T06:13:23.524706Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings),\n    train_labels\n))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), \n                                    list(test_labels))) \n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(val_encodings),\n    val_labels\n))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=50)\nlosss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) # 2e-5\nmodel.compile(optimizer=optimizer, loss=losss, metrics=['accuracy']) # loss=model.compute_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_dataset.shuffle(1000).batch(16), \n          epochs=10)\n          validation_data=val_dataset.shuffle(1000).batch(16))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-tuning Option 2: Using the TFTrainer class","metadata":{}},{"cell_type":"code","source":"from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n\ntraining_args = TFTrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=3,              # total number of training epochs\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=64,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    #logging_dir='./logs',            # directory for storing logs\n)\n\nwith training_args.strategy.scope():\n    trainer_model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=50)\n\ntrainer = TFTrainer(\n    model=trainer_model,                 # the instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset,             # evaluation dataset\n)","metadata":{},"execution_count":null,"outputs":[]}]}
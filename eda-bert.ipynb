{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SCAN (https://scan-interfax.ru/) - это система Международной информационной группы \"Интерфакс\", предназначенная для комплексного решения задач в сфере управления репутацией и анализа эффективности PR. Решает задачи по мониторингу СМИ и соцсетей, анализу медиа-поля, проверке деловой репутации компаний и персон, позволяет пользователям оперативно реагировать на появление негатива в СМИ и соцмедиа, путём осуществления автоматизированного сбора и анализа публичной информации из более чем 60 тысяч источников.\n\nОдной из функциональных возможностей SCAN является фактографический анализ новостной информации - выделение контекстов, в которых упоминается событие или действие некоторого субъекта на заданную тематику и с заданной тональностью.\nНа текущий момент SCAN умеет определять контексты более чем по 800 различным темам. Разработка каждой темы представляет собой длительный процесс по предварительному сбору и анализу большого корпуса текстовой информации для выделения характерных фраз, с последующим написанием машинных правил на специальном DSL (domain-specific language), в котором задействован целый отдел прикладных лингвистов.\n\nМы предлагаем вам принять участие в решении значимой для проекта SCAN задачи, которая позволяет расширить спектр выделяемых системой контекстов и снизит нагрузку на лингвистов:\n\nРазработка средства автоматизированного поиска контекстов на заданные тематики. Нам важна семантическая близость к уже проработанным нами контекстам, чтобы не требовалось описывать каждый контекст в рамках одной тематики машинными правилами на специальном DSL:\n\nВ качестве исходных данных выступают наборы размеченных корпусов на различные тематики.\nВ качестве искомого контекста может выступать как часть предложения исходного текста, так и целое предложение, или даже набор предложений на ту же тему.","metadata":{}},{"cell_type":"code","source":"!pip install pymorphy2","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:47:54.405942Z","iopub.execute_input":"2022-06-24T17:47:54.406516Z","iopub.status.idle":"2022-06-24T17:48:06.840861Z","shell.execute_reply.started":"2022-06-24T17:47:54.406482Z","shell.execute_reply":"2022-06-24T17:48:06.839876Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\n\nimport datatable as dt\nfrom dask import dataframe as dd \n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstopwords_ru = stopwords.words(\"russian\")\nfrom nltk.stem import WordNetLemmatizer\nimport pymorphy2\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\nimport transformers\nimport torch\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer, BertModel, BertConfig","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:48:06.843360Z","iopub.execute_input":"2022-06-24T17:48:06.843779Z","iopub.status.idle":"2022-06-24T17:48:16.847628Z","shell.execute_reply.started":"2022-06-24T17:48:06.843738Z","shell.execute_reply":"2022-06-24T17:48:16.846839Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"## Применение классификации текста с использованием логистической регрессии","metadata":{}},{"cell_type":"code","source":"file = '../input/scan-classification-challange/df_train.csv'\ndf_dt = dt.fread(file)\ndf_dt = df_dt.to_pandas()\ndf_dt.sample(3)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:48:16.849120Z","iopub.execute_input":"2022-06-24T17:48:16.849797Z","iopub.status.idle":"2022-06-24T17:48:17.331547Z","shell.execute_reply.started":"2022-06-24T17:48:16.849760Z","shell.execute_reply":"2022-06-24T17:48:17.330766Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/scan-classification-challange/df_train.csv')\ntrain.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:48:17.333555Z","iopub.execute_input":"2022-06-24T17:48:17.334085Z","iopub.status.idle":"2022-06-24T17:48:17.614855Z","shell.execute_reply.started":"2022-06-24T17:48:17.334046Z","shell.execute_reply":"2022-06-24T17:48:17.613863Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Посмотрим, что из себя представляет значение с текстом\n\ntrain['text'][12]","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:48:17.616174Z","iopub.execute_input":"2022-06-24T17:48:17.616611Z","iopub.status.idle":"2022-06-24T17:48:17.625024Z","shell.execute_reply.started":"2022-06-24T17:48:17.616573Z","shell.execute_reply":"2022-06-24T17:48:17.624067Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# после удаления дубликатов, имеетследующий размер  данных\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:48:17.626571Z","iopub.execute_input":"2022-06-24T17:48:17.626968Z","iopub.status.idle":"2022-06-24T17:48:17.635656Z","shell.execute_reply.started":"2022-06-24T17:48:17.626932Z","shell.execute_reply":"2022-06-24T17:48:17.634738Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Видно, что данные сильно не сбалансированны\n\nfrom collections import Counter\ncounter = Counter(train['class'])\nplt.figure(figsize=(15, 6))\nplt.bar(counter.keys(), counter.values(), width=2)\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:48:17.637371Z","iopub.execute_input":"2022-06-24T17:48:17.637884Z","iopub.status.idle":"2022-06-24T17:48:18.525044Z","shell.execute_reply.started":"2022-06-24T17:48:17.637676Z","shell.execute_reply":"2022-06-24T17:48:18.524282Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/scan-classification-challange/df_test.csv', index_col=0)\ntest.sample(3)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:48:18.526010Z","iopub.execute_input":"2022-06-24T17:48:18.526361Z","iopub.status.idle":"2022-06-24T17:48:18.855611Z","shell.execute_reply.started":"2022-06-24T17:48:18.526318Z","shell.execute_reply":"2022-06-24T17:48:18.854669Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Удаляем дубликаты\n\ntrain.drop_duplicates(subset={'text'}, inplace=True) \ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:48:18.856948Z","iopub.execute_input":"2022-06-24T17:48:18.857426Z","iopub.status.idle":"2022-06-24T17:48:18.963771Z","shell.execute_reply.started":"2022-06-24T17:48:18.857388Z","shell.execute_reply":"2022-06-24T17:48:18.962870Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/scan-classification-challange/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:48:18.966600Z","iopub.execute_input":"2022-06-24T17:48:18.966991Z","iopub.status.idle":"2022-06-24T17:48:18.986388Z","shell.execute_reply.started":"2022-06-24T17:48:18.966955Z","shell.execute_reply":"2022-06-24T17:48:18.985711Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Названия категорий переводим в числовой формат и записываем в отдельный столбец \n\ntrain['encoded_cat'] = train['class'].astype('category').cat.codes\ntrain.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:48:18.987720Z","iopub.execute_input":"2022-06-24T17:48:18.988256Z","iopub.status.idle":"2022-06-24T17:48:19.009464Z","shell.execute_reply.started":"2022-06-24T17:48:18.988217Z","shell.execute_reply":"2022-06-24T17:48:19.008699Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Создадим функцию по очистке данных. Будем переводить слова в нижний регистр, \n# удалять стоп слова, удалять числа и раздичные знаки которые не несут смысловой нагрузки. \n# Все слова преобразуем к их первоначалоной форме (Лемматизация)\n\nmorph = pymorphy2.MorphAnalyzer()\npatterns = \"[A-Z|a-z|0-9!#$%&'()*+,./:“″;”<=>?@[\\]^_`{|}~—\\\"\\-•–«»]+\"\nstops = set(stopwords.words(\"russian\"))\ndef clean(text):\n    text = text.lower()\n    text = re.sub(patterns, ' ', text)\n    tokens = []\n    for token in text.split():\n        if token and token not in stops:\n            token = token.strip()\n            token = morph.normal_forms(token)[0]  # Лемматизация\n            #token = stemmer.stem(token) # Стеммизация\n            tokens.append(token)\n    return ' '.join(tokens)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:48:19.010754Z","iopub.execute_input":"2022-06-24T17:48:19.011168Z","iopub.status.idle":"2022-06-24T17:48:19.251025Z","shell.execute_reply.started":"2022-06-24T17:48:19.011131Z","shell.execute_reply":"2022-06-24T17:48:19.250223Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Применим функцию очистки к train\ntrain['clean'] = train['text'].apply(lambda x: clean(x))\ntrain[['clean', 'text']]","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:48:19.252266Z","iopub.execute_input":"2022-06-24T17:48:19.252633Z","iopub.status.idle":"2022-06-24T17:51:44.438480Z","shell.execute_reply.started":"2022-06-24T17:48:19.252597Z","shell.execute_reply":"2022-06-24T17:51:44.437665Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Применим функцию очистки также для тестовых данных\n\ntest['clean'] = test['text'].apply(lambda x: clean(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:51:44.439970Z","iopub.execute_input":"2022-06-24T17:51:44.440354Z","iopub.status.idle":"2022-06-24T17:54:01.700528Z","shell.execute_reply.started":"2022-06-24T17:51:44.440318Z","shell.execute_reply":"2022-06-24T17:54:01.699688Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Выделим X, y. X - это будет, наш обработанный текст, y -  наш класс\n\ny = train.encoded_cat.values\nX = train.drop(['encoded_cat', 'text'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:54:01.701986Z","iopub.execute_input":"2022-06-24T17:54:01.702358Z","iopub.status.idle":"2022-06-24T17:54:01.712446Z","shell.execute_reply.started":"2022-06-24T17:54:01.702310Z","shell.execute_reply":"2022-06-24T17:54:01.711716Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"## Разделим все данные на train test в соотношении 80/20\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y) # stratify=y","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:54:01.713973Z","iopub.execute_input":"2022-06-24T17:54:01.714466Z","iopub.status.idle":"2022-06-24T17:54:01.751598Z","shell.execute_reply.started":"2022-06-24T17:54:01.714429Z","shell.execute_reply":"2022-06-24T17:54:01.750832Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Преобразуем наши текстовые данны в токены.","metadata":{}},{"cell_type":"code","source":"## CountVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer()\n\nX_train_review_bow = vect.fit_transform(X_train['clean'])\nX_test_review_bow = vect.transform(X_test['clean'])\nX_sub_rev_bow = vect.transform(test['clean'])\n#X_train_review_bow = vect.fit_transform(X_train)\n#X_test_review_bow = vect.transform(X_test)\n\nprint('X_train_review_bow shape: ', X_train_review_bow.shape)\nprint('X_test_review_bow shape: ', X_test_review_bow.shape)\nprint('X_sub_review_bow shape: ', X_sub_rev_bow.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:54:01.752761Z","iopub.execute_input":"2022-06-24T17:54:01.753543Z","iopub.status.idle":"2022-06-24T17:54:03.604301Z","shell.execute_reply.started":"2022-06-24T17:54:01.753503Z","shell.execute_reply":"2022-06-24T17:54:03.602737Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Tf-Idf\nTf-Idf расшифровывается как частота термина, обратная частоте документа, и вместо вычисления количества каждого слова в каждом документе набора данных (Bow) он вычисляет нормализованное количество, где каждое количество слов делится на количество документов, в которых встречается это слово. .\n\nTf-idf(w, d)= Bow(w, d) * log(Общее количество документов/(Количество документов, в которых встречается слово w))\n\nЕсли слово часто встречается в определенном документе, но не во многих других документах, наиболее вероятно, что это слово имеет особое значение для этого документа и получает большее количество, чем раньше, благодаря высокому Idf. С другой стороны, если слово появляется во многих документах, то его Idf близок к 1, а логарифм превращает 1 в 0 и уменьшает его влияние.","metadata":{}},{"cell_type":"code","source":"## Tf-Idf\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\nX_train_review_tfidf = vectorizer.fit_transform(X_train['clean'])\nX_test_review_tfidf = vectorizer.transform(X_test['clean'])\nX_sub_review_tfidf = vectorizer.transform(test['clean'])\n\nprint('X_train_review_tfidf shape: ', X_train_review_tfidf.shape)\nprint('X_test_review_tfidf shape: ', X_test_review_tfidf.shape)\nprint('X_sub_review_tfidf shape: ', X_sub_review_tfidf.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:54:03.607486Z","iopub.execute_input":"2022-06-24T17:54:03.607772Z","iopub.status.idle":"2022-06-24T17:54:05.513317Z","shell.execute_reply.started":"2022-06-24T17:54:03.607746Z","shell.execute_reply":"2022-06-24T17:54:05.511723Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Логистическая регрессия\nПосле создания 80/20 разделения набора данных на поезд-тест я применил логистическую регрессию, которая представляет собой алгоритм классификации, используемый для решения задач бинарной классификации. Классификатор логистической регрессии использует взвешенную комбинацию входных признаков и пропускает их через сигмовидную функцию. Сигмовидная функция преобразует любое введенное вещественное число в число от 0 до 1.","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\ny_test_arg=np.argmax(y_test)\nclf = MultinomialNB()\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow) #prediction from model\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Recall: ', recall_score(y_test, y_pred, average = 'micro'))\nprint('Precision: ', precision_score(y_test, y_pred, average = 'weighted', zero_division = 1))\nprint('F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:54:05.514427Z","iopub.execute_input":"2022-06-24T17:54:05.515307Z","iopub.status.idle":"2022-06-24T17:54:05.687328Z","shell.execute_reply.started":"2022-06-24T17:54:05.515266Z","shell.execute_reply":"2022-06-24T17:54:05.686527Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"clf = MultinomialNB()\ny_test_arg=np.argmax(y_test)\nclf.fit(X_train_review_tfidf, y_train)\n\ny_pred = clf.predict(X_test_review_tfidf)\n#y_pred = clf.predict(X_sub_review_tfidf)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:54:05.688445Z","iopub.execute_input":"2022-06-24T17:54:05.689333Z","iopub.status.idle":"2022-06-24T17:54:05.844914Z","shell.execute_reply.started":"2022-06-24T17:54:05.689273Z","shell.execute_reply":"2022-06-24T17:54:05.844068Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', random_state=42)\nclf.fit(X_train_review_tfidf, y_train)\n\n#y_pred = clf.predict(X_test_review_tfidf)\ny_pred = clf.predict(X_test_review_tfidf)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:54:05.846150Z","iopub.execute_input":"2022-06-24T17:54:05.846734Z","iopub.status.idle":"2022-06-24T17:55:06.743234Z","shell.execute_reply.started":"2022-06-24T17:54:05.846695Z","shell.execute_reply":"2022-06-24T17:55:06.742410Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:55:06.744602Z","iopub.execute_input":"2022-06-24T17:55:06.745219Z","iopub.status.idle":"2022-06-24T17:56:07.717183Z","shell.execute_reply.started":"2022-06-24T17:55:06.745180Z","shell.execute_reply":"2022-06-24T17:56:07.716229Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## SMOTE","metadata":{}},{"cell_type":"code","source":"\"\"\"\"from numpy import mean\nfrom numpy import std\nfrom pandas import read_csv\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\n \n\n# evaluate a model\ndef evaluate_model(X, y, model):\n# define evaluation procedure\n    cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=3, random_state=1)\n# evaluate model\n    scores = cross_val_score(model, X_train_review_tfidf, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n \n# load the dataset\n#X, y = load_dataset(full_path)\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, class_weight='balanced') #`1000\n# evaluate the model\nscores = evaluate_model(X_train_review_tfidf, y_train, model)\n# summarize performance\nprint('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:36:02.638008Z","iopub.execute_input":"2022-06-24T15:36:02.638828Z","iopub.status.idle":"2022-06-24T15:36:02.648153Z","shell.execute_reply.started":"2022-06-24T15:36:02.638789Z","shell.execute_reply":"2022-06-24T15:36:02.647278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ngramm","metadata":{}},{"cell_type":"code","source":"# CountVectorizer\nvect = CountVectorizer(ngram_range=(1, 2))\n\nX_train_review_bow = vect.fit_transform(X_train['clean'])\nX_test_review_bow = vect.transform(X_test['clean'])\nX_sub_rev_bow = vect.transform(test['clean'])","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:47:47.671345Z","iopub.execute_input":"2022-06-24T15:47:47.672038Z","iopub.status.idle":"2022-06-24T15:47:53.680562Z","shell.execute_reply.started":"2022-06-24T15:47:47.671994Z","shell.execute_reply":"2022-06-24T15:47:53.679522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C = 10, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:47:53.681777Z","iopub.execute_input":"2022-06-24T15:47:53.682183Z","iopub.status.idle":"2022-06-24T15:56:02.743197Z","shell.execute_reply.started":"2022-06-24T15:47:53.682147Z","shell.execute_reply":"2022-06-24T15:56:02.741637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HashingVectorizer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import HashingVectorizer\nvectorizer = HashingVectorizer(ngram_range=(1, 3))\n\nX_train_review_hv = vectorizer.fit_transform(X_train['clean'])\nX_test_review_hv = vectorizer.transform(X_test['clean'])\nX_sub_review_hv = vectorizer.transform(test['clean'])","metadata":{"execution":{"iopub.status.busy":"2022-06-24T16:01:17.694974Z","iopub.execute_input":"2022-06-24T16:01:17.695496Z","iopub.status.idle":"2022-06-24T16:01:22.534743Z","shell.execute_reply.started":"2022-06-24T16:01:17.695458Z","shell.execute_reply":"2022-06-24T16:01:22.533636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', random_state=42)\nclf.fit(X_train_review_hv, y_train)\n\ny_pred = clf.predict(X_test_review_hv)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T16:01:29.246932Z","iopub.execute_input":"2022-06-24T16:01:29.247827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Попробуем другие модели из sklearn","metadata":{}},{"cell_type":"code","source":"# C-классификация опорных векторов\n\nfrom sklearn.svm import SVC\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train_review_bow, y_train)\ny_pred_svc = svclassifier.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred_svc))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:56:07.720989Z","iopub.execute_input":"2022-06-24T17:56:07.723413Z","iopub.status.idle":"2022-06-24T18:00:11.954941Z","shell.execute_reply.started":"2022-06-24T17:56:07.723375Z","shell.execute_reply":"2022-06-24T18:00:11.953953Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Классификатор, реализующий голосование k ближайших соседей\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 10)\nknn.fit(X_train_review_bow, y_train)\ny_pred = knn.predict(X_test_review_bow)\n\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:00:11.956386Z","iopub.execute_input":"2022-06-24T18:00:11.956972Z","iopub.status.idle":"2022-06-24T18:00:28.770475Z","shell.execute_reply.started":"2022-06-24T18:00:11.956932Z","shell.execute_reply":"2022-06-24T18:00:28.769620Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Гиперпараметры","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(multi_class='multinomial', penalty='l2', random_state=42)\n#penalty = ['l1','l2']\nsolver = ['newton-cg', 'sag', 'saga', 'lbfgs']\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\nalpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\ngrid = dict(alpha=alpha)\nc_values = [100, 10, 1.0]\n#param_grid = {'C':[1, 10]}\nparam_grid = dict(C=c_values, solver=solver)\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, verbose=10, cv=cv, scoring='accuracy', error_score=0)\ngrid_search.fit(X_train_review_bow, y_train)\n#print(\"Best: %f using %s\" % (grid_search.best_params_))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Попробуем обучить с полученными гипер параметрами ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='newton-cg', penalty='l2', C = 10, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:00:28.771611Z","iopub.execute_input":"2022-06-24T18:00:28.772546Z","iopub.status.idle":"2022-06-24T18:03:01.546616Z","shell.execute_reply.started":"2022-06-24T18:00:28.772504Z","shell.execute_reply":"2022-06-24T18:03:01.545801Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='saga', penalty='l2', C = 100, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:03:01.547721Z","iopub.execute_input":"2022-06-24T18:03:01.548148Z","iopub.status.idle":"2022-06-24T18:03:27.469540Z","shell.execute_reply.started":"2022-06-24T18:03:01.548090Z","shell.execute_reply":"2022-06-24T18:03:27.468497Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":" С подобранными гиперпараметрами получилось улучшить модель до 0.897","metadata":{}},{"cell_type":"markdown","source":"### делаем предсказание на Test и выкладываем на Kaggle","metadata":{}},{"cell_type":"code","source":"clf = MultinomialNB(alpha=1)\nclf.fit(X_train_review_tfidf, y_train)\n\ny_pred = clf.predict(X_sub_review_tfidf)\n#print('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-17T10:38:30.797143Z","iopub.execute_input":"2022-06-17T10:38:30.797844Z","iopub.status.idle":"2022-06-17T10:38:30.841422Z","shell.execute_reply.started":"2022-06-17T10:38:30.797803Z","shell.execute_reply":"2022-06-17T10:38:30.840259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train_review_bow, y_train)\ny_pred_svc = svclassifier.predict(X_sub_review_tfidf)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:46:02.498956Z","iopub.execute_input":"2022-06-17T15:46:02.499398Z","iopub.status.idle":"2022-06-17T16:01:53.196087Z","shell.execute_reply.started":"2022-06-17T15:46:02.49935Z","shell.execute_reply":"2022-06-17T16:01:53.195358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(random_state=42)\nclf.fit(X_train_review_tfidf, y_train)\n\n#y_pred = clf.predict(X_test_review_tfidf)\ny_pred = clf.predict(X_sub_review_tfidf)\n#print('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-16T10:23:37.31609Z","iopub.execute_input":"2022-06-16T10:23:37.317114Z","iopub.status.idle":"2022-06-16T10:24:17.197726Z","shell.execute_reply.started":"2022-06-16T10:23:37.317069Z","shell.execute_reply":"2022-06-16T10:24:17.19669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(multi_class='multinomial', solver='saga', penalty='l2', C = 100, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_sub_rev_bow)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:03:50.058974Z","iopub.execute_input":"2022-06-20T16:03:50.059506Z","iopub.status.idle":"2022-06-20T16:04:35.349235Z","shell.execute_reply.started":"2022-06-20T16:03:50.059463Z","shell.execute_reply":"2022-06-20T16:04:35.348159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(train['class'].unique())\n#list(le.classes_)\nnum = le.transform(train['class'].unique())\n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:04:35.35156Z","iopub.execute_input":"2022-06-20T16:04:35.352292Z","iopub.status.idle":"2022-06-20T16:04:35.370807Z","shell.execute_reply.started":"2022-06-20T16:04:35.352245Z","shell.execute_reply":"2022-06-20T16:04:35.369968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_sub = list(le.inverse_transform(y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:04:35.372062Z","iopub.execute_input":"2022-06-20T16:04:35.372555Z","iopub.status.idle":"2022-06-20T16:04:35.381029Z","shell.execute_reply.started":"2022-06-20T16:04:35.372511Z","shell.execute_reply":"2022-06-20T16:04:35.379896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sub_predict = model.predict(X_sub)\nsample_submission['class'] = y_sub\nsample_submission.to_csv('LR_submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:04:35.382987Z","iopub.execute_input":"2022-06-20T16:04:35.383641Z","iopub.status.idle":"2022-06-20T16:04:35.452187Z","shell.execute_reply.started":"2022-06-20T16:04:35.383596Z","shell.execute_reply":"2022-06-20T16:04:35.451484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bert","metadata":{}},{"cell_type":"markdown","source":"## Fine Tuning DistilBERT TensorFlow","metadata":{}},{"cell_type":"code","source":"#!pip uninstall transformers --yes\n#!pip uninstall h5py\n\n!pip install transformers\n#!pip install h5py==2.10.0\n!pip install pytorch\n!pip install pytorch-transformers","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:03:27.473601Z","iopub.execute_input":"2022-06-24T18:03:27.476047Z","iopub.status.idle":"2022-06-24T18:03:59.354675Z","shell.execute_reply.started":"2022-06-24T18:03:27.476009Z","shell.execute_reply":"2022-06-24T18:03:59.353690Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import DistilBertTokenizer\nfrom transformers import DistilBertModel, DistilBertConfig\nfrom transformers import TFDistilBertForSequenceClassification\nimport tensorflow as tf\n#import pandas as pd\nimport json\nimport gc\n#import numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:03:59.358129Z","iopub.execute_input":"2022-06-24T18:03:59.358435Z","iopub.status.idle":"2022-06-24T18:04:00.417587Z","shell.execute_reply.started":"2022-06-24T18:03:59.358406Z","shell.execute_reply":"2022-06-24T18:04:00.416770Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nX_train_encodings = tokenizer(X_train['clean'].to_list(), truncation=True, padding=True)\ntest_encodings = tokenizer(test['text'].to_list(), truncation=True, padding=True)\n\nX_test_encodings = tokenizer(X_test['clean'].to_list(), truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:04:00.422298Z","iopub.execute_input":"2022-06-24T18:04:00.424508Z","iopub.status.idle":"2022-06-24T18:06:13.012640Z","shell.execute_reply.started":"2022-06-24T18:04:00.424469Z","shell.execute_reply":"2022-06-24T18:06:13.011788Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"y_sub = np.zeros(len(test))\ny_sub.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:06:13.013943Z","iopub.execute_input":"2022-06-24T18:06:13.014302Z","iopub.status.idle":"2022-06-24T18:06:13.028560Z","shell.execute_reply.started":"2022-06-24T18:06:13.014268Z","shell.execute_reply":"2022-06-24T18:06:13.027678Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(X_train_encodings),\n    y_train\n))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(X_test_encodings), \n                                    list(y_test))) \nval_class = np.zeros(len(test))\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(test_encodings),\n    val_class))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:06:13.030054Z","iopub.execute_input":"2022-06-24T18:06:13.030586Z","iopub.status.idle":"2022-06-24T18:09:40.446111Z","shell.execute_reply.started":"2022-06-24T18:06:13.030546Z","shell.execute_reply":"2022-06-24T18:09:40.445229Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=50)\nlosss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) # 2e-5\nmodel.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy']) # loss=model.compute_loss","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:09:40.447543Z","iopub.execute_input":"2022-06-24T18:09:40.447917Z","iopub.status.idle":"2022-06-24T18:09:51.824501Z","shell.execute_reply.started":"2022-06-24T18:09:40.447882Z","shell.execute_reply":"2022-06-24T18:09:51.823790Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"model.fit(train_dataset.shuffle(1000).batch(16), \n          epochs=10,\n          validation_data=test_dataset.shuffle(1000).batch(16))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:12:45.857451Z","iopub.execute_input":"2022-06-24T18:12:45.857893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SCAN (https://scan-interfax.ru/) - это система Международной информационной группы \"Интерфакс\", предназначенная для комплексного решения задач в сфере управления репутацией и анализа эффективности PR. Решает задачи по мониторингу СМИ и соцсетей, анализу медиа-поля, проверке деловой репутации компаний и персон, позволяет пользователям оперативно реагировать на появление негатива в СМИ и соцмедиа, путём осуществления автоматизированного сбора и анализа публичной информации из более чем 60 тысяч источников.\n\nОдной из функциональных возможностей SCAN является фактографический анализ новостной информации - выделение контекстов, в которых упоминается событие или действие некоторого субъекта на заданную тематику и с заданной тональностью.\nНа текущий момент SCAN умеет определять контексты более чем по 800 различным темам. Разработка каждой темы представляет собой длительный процесс по предварительному сбору и анализу большого корпуса текстовой информации для выделения характерных фраз, с последующим написанием машинных правил на специальном DSL (domain-specific language), в котором задействован целый отдел прикладных лингвистов.\n\nМы предлагаем вам принять участие в решении значимой для проекта SCAN задачи, которая позволяет расширить спектр выделяемых системой контекстов и снизит нагрузку на лингвистов:\n\nРазработка средства автоматизированного поиска контекстов на заданные тематики. Нам важна семантическая близость к уже проработанным нами контекстам, чтобы не требовалось описывать каждый контекст в рамках одной тематики машинными правилами на специальном DSL:\n\nВ качестве исходных данных выступают наборы размеченных корпусов на различные тематики.\nВ качестве искомого контекста может выступать как часть предложения исходного текста, так и целое предложение, или даже набор предложений на ту же тему.","metadata":{}},{"cell_type":"code","source":"!pip install pymorphy2","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:51:18.912205Z","iopub.execute_input":"2022-06-25T04:51:18.912581Z","iopub.status.idle":"2022-06-25T04:51:35.597917Z","shell.execute_reply.started":"2022-06-25T04:51:18.912554Z","shell.execute_reply":"2022-06-25T04:51:35.596889Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\n\nimport datatable as dt\nfrom dask import dataframe as dd \n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstopwords_ru = stopwords.words(\"russian\")\nfrom nltk.stem import WordNetLemmatizer\nimport pymorphy2\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\nimport transformers\nimport torch\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer, BertModel, BertConfig","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:51:35.601729Z","iopub.execute_input":"2022-06-25T04:51:35.602150Z","iopub.status.idle":"2022-06-25T04:51:45.790625Z","shell.execute_reply.started":"2022-06-25T04:51:35.602116Z","shell.execute_reply":"2022-06-25T04:51:45.789625Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"## Применение классификации текста с использованием логистической регрессии","metadata":{}},{"cell_type":"code","source":"file = '../input/scan-classification-challange/df_train.csv'\ndf_dt = dt.fread(file)\ndf_dt = df_dt.to_pandas()\ndf_dt.sample(3)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:51:45.791851Z","iopub.execute_input":"2022-06-25T04:51:45.792449Z","iopub.status.idle":"2022-06-25T04:51:46.301906Z","shell.execute_reply.started":"2022-06-25T04:51:45.792420Z","shell.execute_reply":"2022-06-25T04:51:46.300885Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/scan-classification-challange/df_train.csv')\ntrain.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:51:46.303887Z","iopub.execute_input":"2022-06-25T04:51:46.304211Z","iopub.status.idle":"2022-06-25T04:51:46.655917Z","shell.execute_reply.started":"2022-06-25T04:51:46.304184Z","shell.execute_reply":"2022-06-25T04:51:46.654774Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Посмотрим, что из себя представляет значение с текстом\n\ntrain['text'][12]","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:51:46.657395Z","iopub.execute_input":"2022-06-25T04:51:46.658530Z","iopub.status.idle":"2022-06-25T04:51:46.668318Z","shell.execute_reply.started":"2022-06-25T04:51:46.658483Z","shell.execute_reply":"2022-06-25T04:51:46.667351Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Удаляем дубликаты\n\ntrain.drop_duplicates(subset={'text'}, inplace=True) \ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:52:53.801435Z","iopub.execute_input":"2022-06-25T04:52:53.801844Z","iopub.status.idle":"2022-06-25T04:52:53.873058Z","shell.execute_reply.started":"2022-06-25T04:52:53.801813Z","shell.execute_reply":"2022-06-25T04:52:53.871974Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:53:04.009289Z","iopub.execute_input":"2022-06-25T04:53:04.009703Z","iopub.status.idle":"2022-06-25T04:53:04.025842Z","shell.execute_reply.started":"2022-06-25T04:53:04.009670Z","shell.execute_reply":"2022-06-25T04:53:04.025126Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Видно, что данные сильно не сбалансированны\n\nfrom collections import Counter\ncounter = Counter(train['class'])\nplt.figure(figsize=(15, 6))\nplt.bar(counter.keys(), counter.values(), width=2)\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:53:14.456680Z","iopub.execute_input":"2022-06-25T04:53:14.457118Z","iopub.status.idle":"2022-06-25T04:53:15.511956Z","shell.execute_reply.started":"2022-06-25T04:53:14.457084Z","shell.execute_reply":"2022-06-25T04:53:15.510844Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/scan-classification-challange/df_test.csv', index_col=0)\ntest.sample(3)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:53:23.634716Z","iopub.execute_input":"2022-06-25T04:53:23.635252Z","iopub.status.idle":"2022-06-25T04:53:23.880988Z","shell.execute_reply.started":"2022-06-25T04:53:23.635219Z","shell.execute_reply":"2022-06-25T04:53:23.879945Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/scan-classification-challange/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:53:29.656035Z","iopub.execute_input":"2022-06-25T04:53:29.656423Z","iopub.status.idle":"2022-06-25T04:53:29.683534Z","shell.execute_reply.started":"2022-06-25T04:53:29.656392Z","shell.execute_reply":"2022-06-25T04:53:29.682588Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Названия категорий переводим в числовой формат и записываем в отдельный столбец \n\ntrain['encoded_cat'] = train['class'].astype('category').cat.codes\ntrain.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:53:31.750827Z","iopub.execute_input":"2022-06-25T04:53:31.751796Z","iopub.status.idle":"2022-06-25T04:53:31.770635Z","shell.execute_reply.started":"2022-06-25T04:53:31.751744Z","shell.execute_reply":"2022-06-25T04:53:31.769886Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train[['class', 'encoded_cat']].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:54:47.476053Z","iopub.execute_input":"2022-06-25T04:54:47.476483Z","iopub.status.idle":"2022-06-25T04:54:47.499140Z","shell.execute_reply.started":"2022-06-25T04:54:47.476450Z","shell.execute_reply":"2022-06-25T04:54:47.497956Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Создадим функцию по очистке данных. Будем переводить слова в нижний регистр, \n# удалять стоп слова, удалять числа и раздичные знаки которые не несут смысловой нагрузки. \n# Все слова преобразуем к их первоначалоной форме (Лемматизация)\n\nmorph = pymorphy2.MorphAnalyzer()\npatterns = \"[A-Z|a-z|0-9!#$%&'()*+,./:“″;”<=>?@[\\]^_`{|}~—\\\"\\-•–«»]+\"\nstops = set(stopwords.words(\"russian\"))\ndef clean(text):\n    text = text.lower()\n    text = re.sub(patterns, ' ', text)\n    tokens = []\n    for token in text.split():\n        if token and token not in stops:\n            token = token.strip()\n            token = morph.normal_forms(token)[0]  # Лемматизация\n            #token = stemmer.stem(token) # Стеммизация\n            tokens.append(token)\n    return ' '.join(tokens)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:55:06.431055Z","iopub.execute_input":"2022-06-25T04:55:06.431707Z","iopub.status.idle":"2022-06-25T04:55:06.938286Z","shell.execute_reply.started":"2022-06-25T04:55:06.431658Z","shell.execute_reply":"2022-06-25T04:55:06.937135Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Применим функцию очистки к train\ntrain['clean'] = train['text'].apply(lambda x: clean(x))\ntrain[['clean', 'text']]","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:55:08.180091Z","iopub.execute_input":"2022-06-25T04:55:08.180977Z","iopub.status.idle":"2022-06-25T04:59:09.196151Z","shell.execute_reply.started":"2022-06-25T04:55:08.180939Z","shell.execute_reply":"2022-06-25T04:59:09.195134Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Применим функцию очистки также для тестовых данных\n\ntest['clean'] = test['text'].apply(lambda x: clean(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:59:09.198216Z","iopub.execute_input":"2022-06-25T04:59:09.198890Z","iopub.status.idle":"2022-06-25T05:01:51.445970Z","shell.execute_reply.started":"2022-06-25T04:59:09.198840Z","shell.execute_reply":"2022-06-25T05:01:51.444878Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Выделим X, y. X - это будет, наш обработанный текст, y -  наш класс\n\ny = train.encoded_cat.values\nX = train.drop(['encoded_cat', 'text'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:01:51.447317Z","iopub.execute_input":"2022-06-25T05:01:51.447683Z","iopub.status.idle":"2022-06-25T05:01:51.456059Z","shell.execute_reply.started":"2022-06-25T05:01:51.447653Z","shell.execute_reply":"2022-06-25T05:01:51.455223Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"## Разделим все данные на train test в соотношении 80/20\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y) # stratify=y","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:01:51.457931Z","iopub.execute_input":"2022-06-25T05:01:51.458463Z","iopub.status.idle":"2022-06-25T05:01:51.504095Z","shell.execute_reply.started":"2022-06-25T05:01:51.458383Z","shell.execute_reply":"2022-06-25T05:01:51.503023Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Преобразуем наши текстовые данны в токены.","metadata":{}},{"cell_type":"code","source":"## CountVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer()\n\nX_train_review_bow = vect.fit_transform(X_train['clean'])\nX_test_review_bow = vect.transform(X_test['clean'])\nX_sub_rev_bow = vect.transform(test['clean'])\n#X_train_review_bow = vect.fit_transform(X_train)\n#X_test_review_bow = vect.transform(X_test)\n\nprint('X_train_review_bow shape: ', X_train_review_bow.shape)\nprint('X_test_review_bow shape: ', X_test_review_bow.shape)\nprint('X_sub_review_bow shape: ', X_sub_rev_bow.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:01:51.505457Z","iopub.execute_input":"2022-06-25T05:01:51.505824Z","iopub.status.idle":"2022-06-25T05:01:53.852897Z","shell.execute_reply.started":"2022-06-25T05:01:51.505771Z","shell.execute_reply":"2022-06-25T05:01:53.851867Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Tf-Idf\nTf-Idf расшифровывается как частота термина, обратная частоте документа, и вместо вычисления количества каждого слова в каждом документе набора данных (Bow) он вычисляет нормализованное количество, где каждое количество слов делится на количество документов, в которых встречается это слово. .\n\nTf-idf(w, d)= Bow(w, d) * log(Общее количество документов/(Количество документов, в которых встречается слово w))\n\nЕсли слово часто встречается в определенном документе, но не во многих других документах, наиболее вероятно, что это слово имеет особое значение для этого документа и получает большее количество, чем раньше, благодаря высокому Idf. С другой стороны, если слово появляется во многих документах, то его Idf близок к 1, а логарифм превращает 1 в 0 и уменьшает его влияние.","metadata":{}},{"cell_type":"code","source":"## Tf-Idf\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\nX_train_review_tfidf = vectorizer.fit_transform(X_train['clean'])\nX_test_review_tfidf = vectorizer.transform(X_test['clean'])\nX_sub_review_tfidf = vectorizer.transform(test['clean'])\n\nprint('X_train_review_tfidf shape: ', X_train_review_tfidf.shape)\nprint('X_test_review_tfidf shape: ', X_test_review_tfidf.shape)\nprint('X_sub_review_tfidf shape: ', X_sub_review_tfidf.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:01:53.854012Z","iopub.execute_input":"2022-06-25T05:01:53.854331Z","iopub.status.idle":"2022-06-25T05:01:56.271326Z","shell.execute_reply.started":"2022-06-25T05:01:53.854303Z","shell.execute_reply":"2022-06-25T05:01:56.270353Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Логистическая регрессия\nПосле создания 80/20 разделения набора данных на поезд-тест я применил логистическую регрессию, которая представляет собой алгоритм классификации, используемый для решения задач бинарной классификации. Классификатор логистической регрессии использует взвешенную комбинацию входных признаков и пропускает их через сигмовидную функцию. Сигмовидная функция преобразует любое введенное вещественное число в число от 0 до 1.","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\ny_test_arg=np.argmax(y_test)\nclf = MultinomialNB()\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow) #prediction from model\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Recall: ', recall_score(y_test, y_pred, average = 'micro'))\nprint('Precision: ', precision_score(y_test, y_pred, average = 'weighted', zero_division = 1))\nprint('F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:01:56.272580Z","iopub.execute_input":"2022-06-25T05:01:56.272970Z","iopub.status.idle":"2022-06-25T05:01:56.526905Z","shell.execute_reply.started":"2022-06-25T05:01:56.272940Z","shell.execute_reply":"2022-06-25T05:01:56.525867Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"clf = MultinomialNB()\ny_test_arg=np.argmax(y_test)\nclf.fit(X_train_review_tfidf, y_train)\n\ny_pred = clf.predict(X_test_review_tfidf)\n#y_pred = clf.predict(X_sub_review_tfidf)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:01:56.528284Z","iopub.execute_input":"2022-06-25T05:01:56.528713Z","iopub.status.idle":"2022-06-25T05:01:56.715914Z","shell.execute_reply.started":"2022-06-25T05:01:56.528683Z","shell.execute_reply":"2022-06-25T05:01:56.714680Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', random_state=42)\nclf.fit(X_train_review_tfidf, y_train)\n\n#y_pred = clf.predict(X_test_review_tfidf)\ny_pred = clf.predict(X_test_review_tfidf)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:01:56.717395Z","iopub.execute_input":"2022-06-25T05:01:56.717740Z","iopub.status.idle":"2022-06-25T05:03:07.623217Z","shell.execute_reply.started":"2022-06-25T05:01:56.717712Z","shell.execute_reply":"2022-06-25T05:03:07.622227Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:03:07.625380Z","iopub.execute_input":"2022-06-25T05:03:07.625741Z","iopub.status.idle":"2022-06-25T05:04:20.231970Z","shell.execute_reply.started":"2022-06-25T05:03:07.625711Z","shell.execute_reply":"2022-06-25T05:04:20.231230Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## SMOTE","metadata":{}},{"cell_type":"code","source":"\"\"\"\"from numpy import mean\nfrom numpy import std\nfrom pandas import read_csv\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\n \n\n# evaluate a model\ndef evaluate_model(X, y, model):\n# define evaluation procedure\n    cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=3, random_state=1)\n# evaluate model\n    scores = cross_val_score(model, X_train_review_tfidf, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n \n# load the dataset\n#X, y = load_dataset(full_path)\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, class_weight='balanced') #`1000\n# evaluate the model\nscores = evaluate_model(X_train_review_tfidf, y_train, model)\n# summarize performance\nprint('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:36:02.638008Z","iopub.execute_input":"2022-06-24T15:36:02.638828Z","iopub.status.idle":"2022-06-24T15:36:02.648153Z","shell.execute_reply.started":"2022-06-24T15:36:02.638789Z","shell.execute_reply":"2022-06-24T15:36:02.647278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ngramm","metadata":{}},{"cell_type":"code","source":"# CountVectorizer\nvect = CountVectorizer(ngram_range=(1, 2))\n\nX_train_review_bow = vect.fit_transform(X_train['clean'])\nX_test_review_bow = vect.transform(X_test['clean'])\nX_sub_rev_bow = vect.transform(test['clean'])","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:47:47.671345Z","iopub.execute_input":"2022-06-24T15:47:47.672038Z","iopub.status.idle":"2022-06-24T15:47:53.680562Z","shell.execute_reply.started":"2022-06-24T15:47:47.671994Z","shell.execute_reply":"2022-06-24T15:47:53.679522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C = 10, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T15:47:53.681777Z","iopub.execute_input":"2022-06-24T15:47:53.682183Z","iopub.status.idle":"2022-06-24T15:56:02.743197Z","shell.execute_reply.started":"2022-06-24T15:47:53.682147Z","shell.execute_reply":"2022-06-24T15:56:02.741637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HashingVectorizer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import HashingVectorizer\nvectorizer = HashingVectorizer(ngram_range=(1, 3))\n\nX_train_review_hv = vectorizer.fit_transform(X_train['clean'])\nX_test_review_hv = vectorizer.transform(X_test['clean'])\nX_sub_review_hv = vectorizer.transform(test['clean'])","metadata":{"execution":{"iopub.status.busy":"2022-06-24T16:01:17.694974Z","iopub.execute_input":"2022-06-24T16:01:17.695496Z","iopub.status.idle":"2022-06-24T16:01:22.534743Z","shell.execute_reply.started":"2022-06-24T16:01:17.695458Z","shell.execute_reply":"2022-06-24T16:01:22.533636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', random_state=42)\nclf.fit(X_train_review_hv, y_train)\n\ny_pred = clf.predict(X_test_review_hv)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T16:01:29.246932Z","iopub.execute_input":"2022-06-24T16:01:29.247827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Попробуем другие модели из sklearn","metadata":{}},{"cell_type":"code","source":"# C-классификация опорных векторов\n\nfrom sklearn.svm import SVC\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train_review_bow, y_train)\ny_pred_svc = svclassifier.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred_svc))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:04:20.233257Z","iopub.execute_input":"2022-06-25T05:04:20.233845Z","iopub.status.idle":"2022-06-25T05:09:55.803615Z","shell.execute_reply.started":"2022-06-25T05:04:20.233802Z","shell.execute_reply":"2022-06-25T05:09:55.801258Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Классификатор, реализующий голосование k ближайших соседей\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 10)\nknn.fit(X_train_review_bow, y_train)\ny_pred = knn.predict(X_test_review_bow)\n\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:09:55.806978Z","iopub.execute_input":"2022-06-25T05:09:55.807525Z","iopub.status.idle":"2022-06-25T05:10:16.793387Z","shell.execute_reply.started":"2022-06-25T05:09:55.807473Z","shell.execute_reply":"2022-06-25T05:10:16.792247Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## Гиперпараметры","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(multi_class='multinomial', penalty='l2', random_state=42)\n#penalty = ['l1','l2']\nsolver = ['newton-cg', 'sag', 'saga', 'lbfgs']\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\nalpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\ngrid = dict(alpha=alpha)\nc_values = [100, 10, 1.0]\n#param_grid = {'C':[1, 10]}\nparam_grid = dict(C=c_values, solver=solver)\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, verbose=10, cv=cv, scoring='accuracy', error_score=0)\ngrid_search.fit(X_train_review_bow, y_train)\n#print(\"Best: %f using %s\" % (grid_search.best_params_))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Попробуем обучить с полученными гипер параметрами ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='newton-cg', penalty='l2', C = 10, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:10:16.794708Z","iopub.execute_input":"2022-06-25T05:10:16.795439Z","iopub.status.idle":"2022-06-25T05:13:22.455961Z","shell.execute_reply.started":"2022-06-25T05:10:16.795400Z","shell.execute_reply":"2022-06-25T05:13:22.454858Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='saga', penalty='l2', C = 100, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:13:22.458914Z","iopub.execute_input":"2022-06-25T05:13:22.459307Z","iopub.status.idle":"2022-06-25T05:13:56.349053Z","shell.execute_reply.started":"2022-06-25T05:13:22.459277Z","shell.execute_reply":"2022-06-25T05:13:56.347851Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":" С подобранными гиперпараметрами получилось улучшить модель до 0.897","metadata":{}},{"cell_type":"markdown","source":"### делаем предсказание на Test и выкладываем на Kaggle","metadata":{}},{"cell_type":"code","source":"clf = MultinomialNB(alpha=1)\nclf.fit(X_train_review_tfidf, y_train)\n\ny_pred = clf.predict(X_sub_review_tfidf)\n#print('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-17T10:38:30.797143Z","iopub.execute_input":"2022-06-17T10:38:30.797844Z","iopub.status.idle":"2022-06-17T10:38:30.841422Z","shell.execute_reply.started":"2022-06-17T10:38:30.797803Z","shell.execute_reply":"2022-06-17T10:38:30.840259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train_review_bow, y_train)\ny_pred_svc = svclassifier.predict(X_sub_review_tfidf)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:46:02.498956Z","iopub.execute_input":"2022-06-17T15:46:02.499398Z","iopub.status.idle":"2022-06-17T16:01:53.196087Z","shell.execute_reply.started":"2022-06-17T15:46:02.49935Z","shell.execute_reply":"2022-06-17T16:01:53.195358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(random_state=42)\nclf.fit(X_train_review_tfidf, y_train)\n\n#y_pred = clf.predict(X_test_review_tfidf)\ny_pred = clf.predict(X_sub_review_tfidf)\n#print('Test Accuracy: ', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-16T10:23:37.31609Z","iopub.execute_input":"2022-06-16T10:23:37.317114Z","iopub.status.idle":"2022-06-16T10:24:17.197726Z","shell.execute_reply.started":"2022-06-16T10:23:37.317069Z","shell.execute_reply":"2022-06-16T10:24:17.19669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(multi_class='multinomial', solver='saga', penalty='l2', C = 100, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_sub_rev_bow)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:03:50.058974Z","iopub.execute_input":"2022-06-20T16:03:50.059506Z","iopub.status.idle":"2022-06-20T16:04:35.349235Z","shell.execute_reply.started":"2022-06-20T16:03:50.059463Z","shell.execute_reply":"2022-06-20T16:04:35.348159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(train['class'].unique())\n#list(le.classes_)\nnum = le.transform(train['class'].unique())\n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:04:35.35156Z","iopub.execute_input":"2022-06-20T16:04:35.352292Z","iopub.status.idle":"2022-06-20T16:04:35.370807Z","shell.execute_reply.started":"2022-06-20T16:04:35.352245Z","shell.execute_reply":"2022-06-20T16:04:35.369968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_sub = list(le.inverse_transform(y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:04:35.372062Z","iopub.execute_input":"2022-06-20T16:04:35.372555Z","iopub.status.idle":"2022-06-20T16:04:35.381029Z","shell.execute_reply.started":"2022-06-20T16:04:35.372511Z","shell.execute_reply":"2022-06-20T16:04:35.379896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sub_predict = model.predict(X_sub)\nsample_submission['class'] = y_sub\nsample_submission.to_csv('LR_submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T16:04:35.382987Z","iopub.execute_input":"2022-06-20T16:04:35.383641Z","iopub.status.idle":"2022-06-20T16:04:35.452187Z","shell.execute_reply.started":"2022-06-20T16:04:35.383596Z","shell.execute_reply":"2022-06-20T16:04:35.451484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bert","metadata":{}},{"cell_type":"markdown","source":"## Fine Tuning DistilBERT TensorFlow","metadata":{}},{"cell_type":"code","source":"#!pip uninstall transformers --yes\n#!pip uninstall h5py\n\n!pip install transformers\n#!pip install h5py==2.10.0\n!pip install pytorch\n!pip install pytorch-transformers","metadata":{"execution":{"iopub.status.busy":"2022-06-24T18:03:27.473601Z","iopub.execute_input":"2022-06-24T18:03:27.476047Z","iopub.status.idle":"2022-06-24T18:03:59.354675Z","shell.execute_reply.started":"2022-06-24T18:03:27.476009Z","shell.execute_reply":"2022-06-24T18:03:59.353690Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import DistilBertTokenizer\nfrom transformers import DistilBertModel, DistilBertConfig\nfrom transformers import TFDistilBertForSequenceClassification\nimport tensorflow as tf\n#import pandas as pd\nimport json\nimport gc\n#import numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:22:21.094180Z","iopub.execute_input":"2022-06-25T05:22:21.094596Z","iopub.status.idle":"2022-06-25T05:22:22.382445Z","shell.execute_reply.started":"2022-06-25T05:22:21.094564Z","shell.execute_reply":"2022-06-25T05:22:22.381226Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Токенизируем текст \ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nX_train_encodings = tokenizer(X_train['clean'].to_list(), truncation=True, padding=True)\ntest_encodings = tokenizer(test['text'].to_list(), truncation=True, padding=True)\n\nX_test_encodings = tokenizer(X_test['clean'].to_list(), truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:22:22.384244Z","iopub.execute_input":"2022-06-25T05:22:22.384583Z","iopub.status.idle":"2022-06-25T05:24:59.889577Z","shell.execute_reply.started":"2022-06-25T05:22:22.384554Z","shell.execute_reply":"2022-06-25T05:24:59.888619Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"y_sub = np.zeros(len(test))\ny_sub.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:24:59.891304Z","iopub.execute_input":"2022-06-25T05:24:59.891647Z","iopub.status.idle":"2022-06-25T05:24:59.899046Z","shell.execute_reply.started":"2022-06-25T05:24:59.891617Z","shell.execute_reply":"2022-06-25T05:24:59.898201Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# Создает dataset с тонекизированными данными\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(X_train_encodings),\n    y_train\n))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(X_test_encodings), \n                                    list(y_test))) \nval_class = np.zeros(len(test))\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(test_encodings),\n    val_class))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:24:59.900404Z","iopub.execute_input":"2022-06-25T05:24:59.900769Z","iopub.status.idle":"2022-06-25T05:28:39.291343Z","shell.execute_reply.started":"2022-06-25T05:24:59.900738Z","shell.execute_reply":"2022-06-25T05:28:39.290287Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=50)\nlosss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) # 2e-5\nmodel.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy']) # loss=model.compute_loss","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:28:39.293142Z","iopub.execute_input":"2022-06-25T05:28:39.293866Z","iopub.status.idle":"2022-06-25T05:29:16.256690Z","shell.execute_reply.started":"2022-06-25T05:28:39.293821Z","shell.execute_reply":"2022-06-25T05:29:16.255876Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"model.fit(train_dataset.shuffle(1000).batch(16), \n          epochs=1,\n          validation_data=test_dataset.shuffle(1000).batch(16))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:29:16.257739Z","iopub.execute_input":"2022-06-25T05:29:16.258800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
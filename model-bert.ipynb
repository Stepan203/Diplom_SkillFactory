{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pymorphy2","metadata":{"papermill":{"duration":12.700286,"end_time":"2022-06-25T21:33:36.062922","exception":false,"start_time":"2022-06-25T21:33:23.362636","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T05:40:20.599343Z","iopub.execute_input":"2022-07-07T05:40:20.599760Z","iopub.status.idle":"2022-07-07T05:40:34.814598Z","shell.execute_reply.started":"2022-07-07T05:40:20.599727Z","shell.execute_reply":"2022-07-07T05:40:34.813427Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nfrom wordcloud import WordCloud\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstopwords_ru = stopwords.words(\"russian\")\nfrom nltk.stem import WordNetLemmatizer\nimport pymorphy2\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n#import transformers\n#import torch\n#import tensorflow as tf\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\n#from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n#from transformers import BertTokenizer, BertModel, BertConfig\n\nimport transformers\nfrom transformers import DistilBertTokenizer\nfrom transformers import DistilBertModel, DistilBertConfig\nfrom transformers import TFDistilBertForSequenceClassification\nimport tensorflow as tf\nimport json\nimport gc","metadata":{"papermill":{"duration":2.007525,"end_time":"2022-06-25T21:33:38.077176","exception":false,"start_time":"2022-06-25T21:33:36.069651","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T05:40:34.817154Z","iopub.execute_input":"2022-07-07T05:40:34.817513Z","iopub.status.idle":"2022-07-07T05:40:48.537106Z","shell.execute_reply.started":"2022-07-07T05:40:34.817481Z","shell.execute_reply":"2022-07-07T05:40:48.535990Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/scan-classification-challange/df_train.csv')\ntrain.sample(3)","metadata":{"papermill":{"duration":0.684804,"end_time":"2022-06-25T21:33:38.768447","exception":false,"start_time":"2022-06-25T21:33:38.083643","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T05:40:48.538993Z","iopub.execute_input":"2022-07-07T05:40:48.539606Z","iopub.status.idle":"2022-07-07T05:40:49.192433Z","shell.execute_reply.started":"2022-07-07T05:40:48.539572Z","shell.execute_reply":"2022-07-07T05:40:49.191217Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Удалим дубликаты\n\ntrain.drop_duplicates(subset={'text'}, inplace=True) \ntrain.shape","metadata":{"papermill":{"duration":0.128957,"end_time":"2022-06-25T21:33:38.975576","exception":false,"start_time":"2022-06-25T21:33:38.846619","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T05:40:49.194404Z","iopub.execute_input":"2022-07-07T05:40:49.194702Z","iopub.status.idle":"2022-07-07T05:40:49.323938Z","shell.execute_reply.started":"2022-07-07T05:40:49.194675Z","shell.execute_reply":"2022-07-07T05:40:49.323107Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# # Названия категорий переводим в числовой формат и записываем в отдельный столбец\n\ntrain['encoded_cat'] = train['class'].astype('category').cat.codes\ntrain.sample(5)","metadata":{"papermill":{"duration":0.035414,"end_time":"2022-06-25T21:33:40.039851","exception":false,"start_time":"2022-06-25T21:33:40.004437","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T05:40:49.324963Z","iopub.execute_input":"2022-07-07T05:40:49.326040Z","iopub.status.idle":"2022-07-07T05:40:49.346740Z","shell.execute_reply.started":"2022-07-07T05:40:49.326005Z","shell.execute_reply":"2022-07-07T05:40:49.345717Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/scan-classification-challange/df_test.csv', index_col=0)\ntest.sample(3)","metadata":{"papermill":{"duration":0.374147,"end_time":"2022-06-25T21:33:40.462568","exception":false,"start_time":"2022-06-25T21:33:40.088421","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T05:40:49.347787Z","iopub.execute_input":"2022-07-07T05:40:49.348672Z","iopub.status.idle":"2022-07-07T05:40:49.867352Z","shell.execute_reply.started":"2022-07-07T05:40:49.348634Z","shell.execute_reply":"2022-07-07T05:40:49.866060Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Посмотрим на состав имеющихся стоп-слов\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstopwords_ru = stopwords.words(\"russian\")\nprint(stopwords.words(\"russian\"))","metadata":{"papermill":{"duration":0.019642,"end_time":"2022-06-25T21:33:40.623466","exception":false,"start_time":"2022-06-25T21:33:40.603824","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T05:40:49.868735Z","iopub.execute_input":"2022-07-07T05:40:49.869035Z","iopub.status.idle":"2022-07-07T05:40:49.879037Z","shell.execute_reply.started":"2022-07-07T05:40:49.869005Z","shell.execute_reply":"2022-07-07T05:40:49.877811Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Теперь выведем все слова длиной менее 3-х символов и дополним список стоп-слов\n# пробовал искать среди стопслов равных 4-м, но там получаем слова 'газа', 'прав', 'дела', 'Рост'\n# как мне кажется данные слова, могут быть полезны для score\n\nstopworlds_new = set()\nmas_stop = set()\nfor words in train['text']:\n    for i in words.split():\n        if len(i) <= 3:\n            mas_stop.add(i)\nstopworlds_new = set(stopwords_ru).union(mas_stop)","metadata":{"papermill":{"duration":0.33337,"end_time":"2022-06-25T21:33:40.966026","exception":false,"start_time":"2022-06-25T21:33:40.632656","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T05:40:49.880551Z","iopub.execute_input":"2022-07-07T05:40:49.881322Z","iopub.status.idle":"2022-07-07T05:40:50.304258Z","shell.execute_reply.started":"2022-07-07T05:40:49.881289Z","shell.execute_reply":"2022-07-07T05:40:50.303074Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Создадим функцию по очистке данных. Будем переводить слова в нижний регистр, \n# удалять стоп слова, удалять числа и раздичные знаки которые не несут смысловой нагрузки. \n# Все слова преобразуем к их первоначалоной форме (Лемматизация)\n\nmorph = pymorphy2.MorphAnalyzer()\npatterns = \"[A-Z|a-z|0-9!#$%&'()*+,./:“″;”<=>?@[\\]^_`{|}~—\\\"\\-•–«»]+\"\n#stops = set(stopwords.words(\"russian\"))\ndef clean(text):\n    text = text.lower()\n    text = re.sub(patterns, ' ', text)\n    tokens = []\n    for token in text.split():\n        if token and token not in stopworlds_new:\n            token = token.strip()\n            token = morph.normal_forms(token)[0]  # Лемматизация\n            #token = stemmer.stem(token) # Стеммизация\n            tokens.append(token)\n    return ' '.join(tokens)","metadata":{"papermill":{"duration":0.242346,"end_time":"2022-06-25T21:33:41.24453","exception":false,"start_time":"2022-06-25T21:33:41.002184","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T05:40:50.306008Z","iopub.execute_input":"2022-07-07T05:40:50.306689Z","iopub.status.idle":"2022-07-07T05:40:50.780883Z","shell.execute_reply.started":"2022-07-07T05:40:50.306657Z","shell.execute_reply":"2022-07-07T05:40:50.779877Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Применим функцию очистки к train и test\n\ntrain['clean'] = train['text'].apply(lambda x: clean(x))\ntest['clean'] = test['text'].apply(lambda x: clean(x))\ntrain[['clean', 'text']]","metadata":{"papermill":{"duration":143.802204,"end_time":"2022-06-25T21:36:05.057019","exception":false,"start_time":"2022-06-25T21:33:41.254815","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T05:40:50.783983Z","iopub.execute_input":"2022-07-07T05:40:50.784929Z","iopub.status.idle":"2022-07-07T05:47:04.584724Z","shell.execute_reply.started":"2022-07-07T05:40:50.784883Z","shell.execute_reply":"2022-07-07T05:47:04.583523Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## FastText","metadata":{}},{"cell_type":"code","source":"ft_train, ft_test = train_test_split(train, random_state=42, test_size=0.2, stratify = train['class'])","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:04:12.154742Z","iopub.execute_input":"2022-06-30T15:04:12.155068Z","iopub.status.idle":"2022-06-30T15:04:12.260053Z","shell.execute_reply.started":"2022-06-30T15:04:12.15504Z","shell.execute_reply":"2022-06-30T15:04:12.25886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_ft_label(s):\n    return '__label__'+s.replace(',','_').replace(' ','_').replace('-','_')\n\nlabels_dict = {}\nfor g in train['class']:\n    labels_dict[to_ft_label(g)] = g","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:45.660867Z","iopub.execute_input":"2022-06-26T13:36:45.662004Z","iopub.status.idle":"2022-06-26T13:36:45.708407Z","shell.execute_reply.started":"2022-06-26T13:36:45.661959Z","shell.execute_reply":"2022-06-26T13:36:45.707424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import remove_stopwords\ntrain.iloc[:, 0] = train.iloc[:, 0].apply(lambda x: ' '.join(simple_preprocess(x)))\ntest.iloc[:, 1] = test.iloc[:, 1].apply(lambda x: ' '.join(simple_preprocess(x)))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:45.710299Z","iopub.execute_input":"2022-06-26T13:36:45.711134Z","iopub.status.idle":"2022-06-26T13:36:47.999185Z","shell.execute_reply.started":"2022-06-26T13:36:45.711084Z","shell.execute_reply":"2022-06-26T13:36:47.998068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = ['class', 'text']\n\n# train\ntrain_for_ft = ft_train[col]\ntrain_for_ft['class']=[to_ft_label(s) for s in train_for_ft['class']]\n\n# test\ntest_for_ft = ft_test[col]\ntest_for_ft['class']=[to_ft_label(s) for s in test_for_ft['class']]","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:48.000668Z","iopub.execute_input":"2022-06-26T13:36:48.00109Z","iopub.status.idle":"2022-06-26T13:36:48.058371Z","shell.execute_reply.started":"2022-06-26T13:36:48.001054Z","shell.execute_reply":"2022-06-26T13:36:48.056437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_for_ft.to_csv('train_for_ft.csv', index=False, sep=' ', header=False, escapechar=\" \")\ntest_for_ft.to_csv('test_for_ft.csv', index=False, sep=' ', header=False, escapechar=\" \")","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:48.0598Z","iopub.execute_input":"2022-06-26T13:36:48.060158Z","iopub.status.idle":"2022-06-26T13:36:48.420534Z","shell.execute_reply.started":"2022-06-26T13:36:48.060125Z","shell.execute_reply":"2022-06-26T13:36:48.419765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import fasttext\nmodel = fasttext.train_supervised('train_for_ft.csv', lr = 0.9)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:48.421701Z","iopub.execute_input":"2022-06-26T13:36:48.422323Z","iopub.status.idle":"2022-06-26T13:36:51.020585Z","shell.execute_reply.started":"2022-06-26T13:36:48.422287Z","shell.execute_reply":"2022-06-26T13:36:51.019098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.test('test_for_ft.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:51.0218Z","iopub.execute_input":"2022-06-26T13:36:51.022122Z","iopub.status.idle":"2022-06-26T13:36:51.219336Z","shell.execute_reply.started":"2022-06-26T13:36:51.022093Z","shell.execute_reply":"2022-06-26T13:36:51.218144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(test):\n    return labels_dict[ model.predict(test['text'], k=1)[0][0] ]\ntest['predictions'] = test.apply(predict,axis=1)\n\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:51.220781Z","iopub.execute_input":"2022-06-26T13:36:51.221702Z","iopub.status.idle":"2022-06-26T13:36:52.59803Z","shell.execute_reply.started":"2022-06-26T13:36:51.221663Z","shell.execute_reply":"2022-06-26T13:36:52.596864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'id':range(0, len(test)),\n                           'class':test['predictions'].values},\n                          columns=['id', 'class'])\nsubmission.to_csv('submission1.csv', index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:52.599332Z","iopub.execute_input":"2022-06-26T13:36:52.599686Z","iopub.status.idle":"2022-06-26T13:36:52.658191Z","shell.execute_reply.started":"2022-06-26T13:36:52.599652Z","shell.execute_reply":"2022-06-26T13:36:52.656834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sub_predict = model.predict([text_sub_sequences, X_sub])\n#sample_submission['class'] = sub_predict_nn2[:,0]\n#sample_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:52.663526Z","iopub.execute_input":"2022-06-26T13:36:52.663928Z","iopub.status.idle":"2022-06-26T13:36:52.668714Z","shell.execute_reply.started":"2022-06-26T13:36:52.663893Z","shell.execute_reply":"2022-06-26T13:36:52.667629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model sklearn\nLogistic Regression","metadata":{}},{"cell_type":"code","source":"# Выделим X, y. X - это будет, наш обработанный текст, y -  наш класс\n\ny = train.encoded_cat.values\nX = train.drop(['encoded_cat', 'text'], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-07T05:47:04.587074Z","iopub.execute_input":"2022-07-07T05:47:04.587443Z","iopub.status.idle":"2022-07-07T05:47:04.595046Z","shell.execute_reply.started":"2022-07-07T05:47:04.587410Z","shell.execute_reply":"2022-07-07T05:47:04.594263Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"## Разделим все данные на train test в соотношении 80/20\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T05:47:04.596452Z","iopub.execute_input":"2022-07-07T05:47:04.596778Z","iopub.status.idle":"2022-07-07T05:47:04.647359Z","shell.execute_reply.started":"2022-07-07T05:47:04.596749Z","shell.execute_reply":"2022-07-07T05:47:04.646218Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Необходимо преобразовать тест в токены, сделаем несколькими способами и посмотрим на результат","metadata":{}},{"cell_type":"code","source":"## CountVectorizer преобразует текст в матрицу количества токенов\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer()\n\nX_train_review_bow = vect.fit_transform(X_train['clean'])\nX_test_review_bow = vect.transform(X_test['clean'])\nX_sub_rev_bow = vect.transform(test['clean'])\n#X_train_review_bow = vect.fit_transform(X_train)\n#X_test_review_bow = vect.transform(X_test)\n\nprint('X_train_review_bow shape: ', X_train_review_bow.shape)\nprint('X_test_review_bow shape: ', X_test_review_bow.shape)\nprint('X_sub_review_bow shape: ', X_sub_rev_bow.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T05:47:04.650507Z","iopub.execute_input":"2022-07-07T05:47:04.651374Z","iopub.status.idle":"2022-07-07T05:47:06.918311Z","shell.execute_reply.started":"2022-07-07T05:47:04.651327Z","shell.execute_reply":"2022-07-07T05:47:06.917152Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"## Tf-Idf преобразует текст в матрицу функций TF-IDF (частота обратная частоте документа)\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\nX_train_review_tfidf = vectorizer.fit_transform(X_train['clean'])\nX_test_review_tfidf = vectorizer.transform(X_test['clean'])\nX_sub_review_tfidf = vectorizer.transform(test['clean'])\n\nprint('X_train_review_tfidf shape: ', X_train_review_tfidf.shape)\nprint('X_test_review_tfidf shape: ', X_test_review_tfidf.shape)\nprint('X_sub_review_tfidf shape: ', X_sub_review_tfidf.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T05:47:06.920276Z","iopub.execute_input":"2022-07-07T05:47:06.921107Z","iopub.status.idle":"2022-07-07T05:47:09.239728Z","shell.execute_reply.started":"2022-07-07T05:47:06.921057Z","shell.execute_reply":"2022-07-07T05:47:09.238399Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Построим модель логистической регресии, в качестве метрики будем выводить accuracy и F1\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\n\nclf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', random_state=42)\nclf.fit(X_train_review_tfidf, y_train)\n\n#y_pred = clf.predict(X_test_review_tfidf)\ny_pred = clf.predict(X_test_review_tfidf)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-07-07T05:47:09.241548Z","iopub.execute_input":"2022-07-07T05:47:09.242276Z","iopub.status.idle":"2022-07-07T05:48:18.871656Z","shell.execute_reply.started":"2022-07-07T05:47:09.242229Z","shell.execute_reply":"2022-07-07T05:48:18.870478Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-07-07T05:48:18.873447Z","iopub.execute_input":"2022-07-07T05:48:18.873784Z","iopub.status.idle":"2022-07-07T05:49:29.785403Z","shell.execute_reply.started":"2022-07-07T05:48:18.873754Z","shell.execute_reply":"2022-07-07T05:49:29.784321Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Попробуем другие модели sklearn","metadata":{}},{"cell_type":"code","source":"# C-классификация опорных векторов\n\nfrom sklearn.svm import SVC\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train_review_bow, y_train)\ny_pred_svc = svclassifier.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred_svc))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-07-07T05:49:29.786731Z","iopub.execute_input":"2022-07-07T05:49:29.787038Z","iopub.status.idle":"2022-07-07T05:54:34.647491Z","shell.execute_reply.started":"2022-07-07T05:49:29.787010Z","shell.execute_reply":"2022-07-07T05:54:34.645743Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Классификатор, реализующий голосование k ближайших соседей\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 10)\nknn.fit(X_train_review_bow, y_train)\ny_pred = knn.predict(X_test_review_bow)\n\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-07-07T05:54:34.649825Z","iopub.execute_input":"2022-07-07T05:54:34.650359Z","iopub.status.idle":"2022-07-07T05:54:53.030529Z","shell.execute_reply.started":"2022-07-07T05:54:34.650305Z","shell.execute_reply":"2022-07-07T05:54:53.029080Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Гипер параметры","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n\"\"\"model = LogisticRegression(multi_class='multinomial', penalty='l2', random_state=42)\n#penalty = ['l1','l2']\nsolver = ['newton-cg', 'sag', 'saga', 'lbfgs']\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\nalpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\ngrid = dict(alpha=alpha)\nc_values = [100, 10, 1.0]\n#param_grid = {'C':[1, 10]}\nparam_grid = dict(C=c_values, solver=solver)\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, verbose=10, cv=cv, scoring='accuracy', error_score=0)\ngrid_search.fit(X_train_review_bow, y_train)\n#print(\"Best: %f using %s\" % (grid_search.best_params_))\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-30T06:34:20.960537Z","iopub.execute_input":"2022-06-30T06:34:20.960901Z","iopub.status.idle":"2022-06-30T06:34:20.969802Z","shell.execute_reply.started":"2022-06-30T06:34:20.960871Z","shell.execute_reply":"2022-06-30T06:34:20.968694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='newton-cg', penalty='l2', C = 10, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-07-07T05:54:53.033598Z","iopub.execute_input":"2022-07-07T05:54:53.033918Z","iopub.status.idle":"2022-07-07T05:57:25.900896Z","shell.execute_reply.started":"2022-07-07T05:54:53.033889Z","shell.execute_reply":"2022-07-07T05:57:25.899468Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='saga', penalty='l2', C = 100, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-07-07T05:57:25.902443Z","iopub.execute_input":"2022-07-07T05:57:25.902898Z","iopub.status.idle":"2022-07-07T05:58:00.545423Z","shell.execute_reply.started":"2022-07-07T05:57:25.902860Z","shell.execute_reply":"2022-07-07T05:58:00.544260Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Стекинг","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\n#from sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\n\n#data, target = load_breast_cancer(return_X_y=True)\n\nestimators = [('lr', LogisticRegression()), ('svc', SVC(kernel='linear'))]\nmodelClf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n\n#X_train, X_valid, y_train, y_valid = train_test_split(data, target, test_size=0.3, random_state=12)\n\nmodelClf.fit(X_train_review_bow, y_train)\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))\n#print(modelClf.score(X_valid, y_valid))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T06:37:06.848192Z","iopub.execute_input":"2022-06-30T06:37:06.849159Z","iopub.status.idle":"2022-06-30T07:00:09.912727Z","shell.execute_reply.started":"2022-06-30T06:37:06.849099Z","shell.execute_reply":"2022-06-30T07:00:09.907524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bert TenzorFlow","metadata":{}},{"cell_type":"code","source":"!pip install transformers\n!pip install pytorch\n!pip install pytorch-transformers","metadata":{"execution":{"iopub.status.busy":"2022-07-07T05:58:00.546922Z","iopub.execute_input":"2022-07-07T05:58:00.548126Z","iopub.status.idle":"2022-07-07T05:58:38.838753Z","shell.execute_reply.started":"2022-07-07T05:58:00.548076Z","shell.execute_reply":"2022-07-07T05:58:38.837154Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import DistilBertTokenizer\nfrom transformers import DistilBertModel, DistilBertConfig\nfrom transformers import TFDistilBertForSequenceClassification\nimport tensorflow as tf\nimport json\nimport gc","metadata":{"execution":{"iopub.status.busy":"2022-07-07T05:58:38.840546Z","iopub.execute_input":"2022-07-07T05:58:38.841647Z","iopub.status.idle":"2022-07-07T05:58:38.851667Z","shell.execute_reply.started":"2022-07-07T05:58:38.841590Z","shell.execute_reply":"2022-07-07T05:58:38.850739Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Токенизируем текст \n\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nX_train_encodings = tokenizer(X_train['clean'].to_list(), truncation=True, padding=True)\ntest_encodings = tokenizer(test['text'].to_list(), truncation=True, padding=True)\n\nX_test_encodings = tokenizer(X_test['clean'].to_list(), truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T17:00:25.228188Z","iopub.execute_input":"2022-07-02T17:00:25.228582Z","iopub.status.idle":"2022-07-02T17:00:26.994262Z","shell.execute_reply.started":"2022-07-02T17:00:25.228551Z","shell.execute_reply":"2022-07-02T17:00:26.993091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создает dataset с тонекизированными данными\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(X_train_encodings),\n    y_train\n))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(X_test_encodings), \n                                    list(y_test))) \nval_class = np.zeros(len(test))\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(test_encodings),\n    val_class))","metadata":{"execution":{"iopub.status.busy":"2022-07-02T17:00:26.995295Z","iopub.status.idle":"2022-07-02T17:00:26.995842Z","shell.execute_reply.started":"2022-07-02T17:00:26.995657Z","shell.execute_reply":"2022-07-02T17:00:26.995676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class F1_Score(tf.keras.metrics.Metric):\n\n    def __init__(self, name='f1_score', **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.f1 = self.add_weight(name='f1', initializer='zeros')\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        p = Precision(thresholds=0.5)(y_true, y_pred)\n        r = Recall(thresholds=0.5)(y_true, y_pred)\n        self.f1 = 2 * ((p * r) / (p + r + 1e-6))\n\n    def result(self):\n        return self.f1\n\n    def reset_states(self):\n        self.f1.assign(0)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T17:00:27.784003Z","iopub.execute_input":"2022-07-02T17:00:27.784736Z","iopub.status.idle":"2022-07-02T17:00:27.792517Z","shell.execute_reply.started":"2022-07-02T17:00:27.7847Z","shell.execute_reply":"2022-07-02T17:00:27.791399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\n\ndef recall_m(y_train, y_test):\n    true_positives = K.sum(K.round(K.clip(y_train * y_test, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_train, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_train, y_test):\n    true_positives = K.sum(K.round(K.clip(y_train * y_test, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_test, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_train, y_test):\n    precision = precision_m(y_train, y_test)\n    recall = recall_m(y_train, y_test)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","metadata":{"execution":{"iopub.status.busy":"2022-07-02T09:18:53.022001Z","iopub.execute_input":"2022-07-02T09:18:53.022474Z","iopub.status.idle":"2022-07-02T09:18:53.031346Z","shell.execute_reply.started":"2022-07-02T09:18:53.022438Z","shell.execute_reply":"2022-07-02T09:18:53.030317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=50)\nlosss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5) # 2e-5\nmodel.compile(optimizer=optimizer, loss=losss, metrics=['accuracy']) # loss=model.compute_loss #model.hf_compute_loss()","metadata":{"execution":{"iopub.status.busy":"2022-07-02T16:09:36.387552Z","iopub.execute_input":"2022-07-02T16:09:36.388001Z","iopub.status.idle":"2022-07-02T16:09:39.781581Z","shell.execute_reply.started":"2022-07-02T16:09:36.387963Z","shell.execute_reply":"2022-07-02T16:09:39.780568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Замораживаем слои\n#model.base_model.transformer.layer\nfor layers in model.layers[:]:\n    #layers.trainable = False\n    print(layers)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T16:10:43.184939Z","iopub.execute_input":"2022-07-02T16:10:43.185359Z","iopub.status.idle":"2022-07-02T16:10:43.193604Z","shell.execute_reply.started":"2022-07-02T16:10:43.185326Z","shell.execute_reply":"2022-07-02T16:10:43.19237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.compile(optimizer=optimizer, loss=losss, metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-07-02T09:19:43.684182Z","iopub.execute_input":"2022-07-02T09:19:43.685185Z","iopub.status.idle":"2022-07-02T09:19:43.695994Z","shell.execute_reply.started":"2022-07-02T09:19:43.685149Z","shell.execute_reply":"2022-07-02T09:19:43.695092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_dataset.shuffle(1000).batch(16), \n          epochs=1,\n          validation_data=test_dataset.shuffle(1000).batch(16))","metadata":{"execution":{"iopub.status.busy":"2022-07-02T09:24:10.380534Z","iopub.execute_input":"2022-07-02T09:24:10.380894Z","iopub.status.idle":"2022-07-02T09:24:59.534494Z","shell.execute_reply.started":"2022-07-02T09:24:10.380863Z","shell.execute_reply":"2022-07-02T09:24:59.532809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distill-Bert fine-tuning - Huggingface and Pytorch","metadata":{}},{"cell_type":"code","source":"!pip install datasets\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-07-02T16:51:23.981938Z","iopub.execute_input":"2022-07-02T16:51:23.982316Z","iopub.status.idle":"2022-07-02T16:51:35.103634Z","shell.execute_reply.started":"2022-07-02T16:51:23.982284Z","shell.execute_reply":"2022-07-02T16:51:35.102331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id2label = {str(i): label for i, label in enumerate(train[\"class\"].unique().tolist())}\nlabel2id = {v: k for k, v in id2label.items()}\nprint(label2id)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T16:51:35.105435Z","iopub.execute_input":"2022-07-02T16:51:35.106441Z","iopub.status.idle":"2022-07-02T16:51:35.119366Z","shell.execute_reply.started":"2022-07-02T16:51:35.106406Z","shell.execute_reply":"2022-07-02T16:51:35.118397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Токенизатор DistilBERT будет принимать обучающие данные только в том случае, если у него есть labels столбец, \n# поэтому мы добавим еще один столбец с нашими сопоставленными метками.\n\ntrain = (train.assign(labels=train[\"class\"].map(label2id)))\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-02T16:51:35.121615Z","iopub.execute_input":"2022-07-02T16:51:35.122247Z","iopub.status.idle":"2022-07-02T16:51:35.167791Z","shell.execute_reply.started":"2022-07-02T16:51:35.122209Z","shell.execute_reply":"2022-07-02T16:51:35.166695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создаем Dataset и разбиваем на test и train\n\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\n\ndataset = Dataset.from_pandas(train).train_test_split(train_size=0.8, seed=123)\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T16:51:35.169105Z","iopub.execute_input":"2022-07-02T16:51:35.169409Z","iopub.status.idle":"2022-07-02T16:51:35.346636Z","shell.execute_reply.started":"2022-07-02T16:51:35.169381Z","shell.execute_reply":"2022-07-02T16:51:35.345445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#\ndataset = dataset.class_encode_column(\"labels\")","metadata":{"execution":{"iopub.status.busy":"2022-07-02T16:51:35.348355Z","iopub.execute_input":"2022-07-02T16:51:35.348702Z","iopub.status.idle":"2022-07-02T16:51:41.590784Z","shell.execute_reply.started":"2022-07-02T16:51:35.348671Z","shell.execute_reply":"2022-07-02T16:51:41.589787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# загружаем трансформатор AutoTokenizer для DistilBERT\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2022-07-02T16:51:41.592773Z","iopub.execute_input":"2022-07-02T16:51:41.593221Z","iopub.status.idle":"2022-07-02T16:51:46.8248Z","shell.execute_reply.started":"2022-07-02T16:51:41.593179Z","shell.execute_reply":"2022-07-02T16:51:46.823561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Составляем список столбцов, которые нужно удалить из набора данных при токенизации\n\ncols_to_remove = [col for col in dataset[\"train\"].column_names if col != \"labels\"]\nprint(cols_to_remove)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T16:51:46.826282Z","iopub.execute_input":"2022-07-02T16:51:46.826629Z","iopub.status.idle":"2022-07-02T16:51:46.832703Z","shell.execute_reply.started":"2022-07-02T16:51:46.82659Z","shell.execute_reply":"2022-07-02T16:51:46.831085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# После завершения токенизации мы подготавливаем набор данных для передачи в модель, устанавливая его формат \"torch\"\n\ndef tokenize(batch):\n    tokenized_batch = tokenizer(batch['text'], padding=True, truncation=True, max_length=320)\n    tokenized_batch['labels'] = [int(label) for label in batch[\"labels\"]]\n    return tokenized_batch\n\ndataset_enc = dataset.map(tokenize, batched=True, remove_columns=cols_to_remove, num_proc=4)\ndataset_enc.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n\nprint(dataset_enc[\"train\"].column_names)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T16:51:46.83442Z","iopub.execute_input":"2022-07-02T16:51:46.834862Z","iopub.status.idle":"2022-07-02T16:52:00.152888Z","shell.execute_reply.started":"2022-07-02T16:51:46.834824Z","shell.execute_reply":"2022-07-02T16:52:00.151819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создаем экземпляр a DataLoader для каждого разделения набора данных, чтобы передать его модели\n\nfrom transformers import DataCollatorWithPadding\nfrom torch.utils.data import DataLoader\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrain_dataloader = DataLoader(\n    dataset_enc[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n)\neval_dataloader = DataLoader(\n    dataset_enc[\"test\"], batch_size=8, collate_fn=data_collator\n)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T16:52:00.157764Z","iopub.execute_input":"2022-07-02T16:52:00.158203Z","iopub.status.idle":"2022-07-02T16:52:00.189828Z","shell.execute_reply.started":"2022-07-02T16:52:00.158164Z","shell.execute_reply":"2022-07-02T16:52:00.188745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Загружаем модель\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoModelForSequenceClassification\n\n# Load model from checkpoint\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=50)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T16:52:00.191229Z","iopub.execute_input":"2022-07-02T16:52:00.1916Z","iopub.status.idle":"2022-07-02T16:52:15.518338Z","shell.execute_reply.started":"2022-07-02T16:52:00.191564Z","shell.execute_reply":"2022-07-02T16:52:15.517225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Определяем гтперпараметры, оптимизатор, скорость, планировщик\n\nfrom transformers import AdamW\nfrom transformers import get_scheduler\nfrom transformers import get_linear_schedule_with_warmup\n\nlearning_rate = 2e-5\nnum_epochs = 5\n\n# Гиперпараметры\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]\noptimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n\n#optimizer = AdamW(model.parameters(), lr=learning_rate)\n\nnum_training_batches = len(train_dataloader)\nnum_training_steps = num_epochs * num_training_batches\nlr_scheduler = get_scheduler(\n    \"linear\",                   \n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:58:35.417697Z","iopub.execute_input":"2022-07-02T13:58:35.418821Z","iopub.status.idle":"2022-07-02T13:58:35.442486Z","shell.execute_reply.started":"2022-07-02T13:58:35.418765Z","shell.execute_reply":"2022-07-02T13:58:35.441635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Перемещаем модель на GPU\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T11:50:42.022003Z","iopub.execute_input":"2022-07-02T11:50:42.02295Z","iopub.status.idle":"2022-07-02T11:50:42.037584Z","shell.execute_reply.started":"2022-07-02T11:50:42.022909Z","shell.execute_reply":"2022-07-02T11:50:42.036524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Запускаем обучение модели\n\nfrom tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(num_training_steps))\n\n# Train the model with PyTorch training loop\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:15:20.19718Z","iopub.execute_input":"2022-07-02T13:15:20.197611Z","iopub.status.idle":"2022-07-02T13:44:50.742167Z","shell.execute_reply.started":"2022-07-02T13:15:20.197575Z","shell.execute_reply":"2022-07-02T13:44:50.741219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Оценка модели\n\nfrom datasets import load_metric\n\n# Load metric\n#metric = load_metric(\"glue\", \"mrpc\")\nmetric = load_metric(\"f1\")\n# Iteratively evaluate the model and compute metrics\nmodel.eval()\nfor batch in eval_dataloader:\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**batch)\n\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n\n# Get model accuracy and F1 score\nmetric.compute(predictions=predictions, references=batch[\"labels\"], average=\"weighted\")\n#metric.compute(predictions=predictions, references=batch[\"labels\"],average=\"weighted\")","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:44:50.744117Z","iopub.execute_input":"2022-07-02T13:44:50.744706Z","iopub.status.idle":"2022-07-02T13:46:08.006257Z","shell.execute_reply.started":"2022-07-02T13:44:50.744667Z","shell.execute_reply":"2022-07-02T13:46:08.005207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DistilBert Freeze","metadata":{}},{"cell_type":"code","source":"def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n    acc_list = []\n    for i in range(y_true.shape[0]):\n        set_true = set( np.where(y_true[i])[0] )\n        set_pred = set( np.where(y_pred[i])[0] )\n        tmp_a = None\n        if len(set_true) == 0 and len(set_pred) == 0:\n            tmp_a = 1\n        else:\n            tmp_a = len(set_true.intersection(set_pred))/\\\n                    float( len(set_true.union(set_pred)) )\n        acc_list.append(tmp_a)\n    return np.mean(acc_list)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:14:54.8539Z","iopub.execute_input":"2022-06-29T17:14:54.854383Z","iopub.status.idle":"2022-06-29T17:14:55.067433Z","shell.execute_reply.started":"2022-06-29T17:14:54.854333Z","shell.execute_reply":"2022-06-29T17:14:55.066535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Применяем get_dumies для таргета\ntrain_class = pd.get_dummies(train, columns=['class'])\ntrain_class.sample(3)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:15:10.556839Z","iopub.execute_input":"2022-06-29T17:15:10.557219Z","iopub.status.idle":"2022-06-29T17:15:11.980424Z","shell.execute_reply.started":"2022-06-29T17:15:10.557186Z","shell.execute_reply":"2022-06-29T17:15:11.978484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['labels'] = train_class.iloc[:, 2:].values.tolist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 372\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 32\nEPOCHS = 8\nLEARNING_RATE = 2e-05\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)\n\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nfrom transformers import BertModel\n\nclass MultiLabelDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.text = dataframe.text_clean\n        self.targets = self.data.labels\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = str(self.text[index])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создаем dataset и dataloader для нейронной сети\n\ntrain_size = 0.8\ntrain_data=train.sample(frac=train_size,random_state=200)\ntest_data=train.drop(train_data.index).reset_index(drop=True)\ntrain_data = train_data.reset_index(drop=True)\n\n\nprint(\"FULL Dataset: {}\".format(train.shape))\nprint(\"TRAIN Dataset: {}\".format(train_data.shape))\nprint(\"TEST Dataset: {}\".format(test_data.shape))\n\ntraining_set = MultiLabelDataset(train_data, tokenizer, MAX_LEN)\ntesting_set = MultiLabelDataset(test_data, tokenizer, MAX_LEN)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# замораживаем слои\n\nfor param in distilbert.parameters():\n  param.requires_grad = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создаем Class для нашей модели\n# тут добавляем слой Dropuot чтоб не переобучиться, а также 2 Linear\n\nclass DistilBERTClass(torch.nn.Module):\n    def __init__(self):\n        super(DistilBERTClass, self).__init__()\n        self.distilbert = distilbert\n        #self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        \n        self.dropout = torch.nn.Dropout(0.1)\n        \n        self.relu = nn.ReLU()\n        self.pre_classifier = torch.nn.Linear(768, 768)\n        self.classifier = torch.nn.Linear(768, 50)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_1 = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_state = output_1[0]\n        pooler = hidden_state[:, 0]\n        pooler = self.pre_classifier(pooler)\n        pooler = self.relu(pooler)\n        #pooler = torch.nn.Tanh()(pooler)\n        pooler = self.dropout(pooler)\n        output = self.classifier(pooler)\n        return output\n\nmodel = DistilBERTClass()\nmodel.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\noptimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(epoch):\n    model.train()\n    for _,data in tqdm(enumerate(training_loader, 0)):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.float)\n\n        outputs = model(ids, mask, token_type_ids)\n\n        optimizer.zero_grad()\n        loss = loss_fn(outputs, targets)\n        if _%5000==0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        \n        loss.backward()\n        optimizer.step()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    train(epoch)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validation(testing_loader):\n    model.eval()\n    fin_targets=[]\n    fin_outputs=[]\n    with torch.no_grad():\n        for _, data in tqdm(enumerate(testing_loader, 0)):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.float)\n            outputs = model(ids, mask, token_type_ids)\n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return fin_outputs, fin_targets","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs, targets = validation(testing_loader)\n\nfinal_outputs = np.array(outputs) >=0.5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_hamming_loss = metrics.hamming_loss(targets, final_outputs)\nval_hamming_score = hamming_score(np.array(targets), np.array(final_outputs))\nf1_m = f1_score(targets, final_outputs, average='weighted')\nprint(f\"Hamming Score = {val_hamming_score}\")\nprint(f\"Hamming Loss = {val_hamming_loss}\")\nprint(f1_m)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DistilBERTClass(torch.nn.Module):\n    def __init__(self):\n        super(DistilBERTClass, self).__init__()\n        self.distilbert = distilbert\n        #self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        \n        self.dropout = torch.nn.Dropout(0.1)\n        self.rnn = nn.GRU(312,\n                        256,\n                        num_layers=2,\n                        batch_first=True,\n                        dropout=0.1)\n        #self.relu =  nn.ReLU()\n        #self.pre_classifier = torch.nn.Linear(768, 768)\n        self.classifier = torch.nn.Linear(768, 50)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_1 = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_state = output_1[0]\n        pooler = hidden_state[:, 0]\n        #pooler = self.pre_classifier(pooler)\n        #pooler = self.relu(pooler)\n        #pooler = torch.nn.Tanh()(pooler)\n        pooler = self.dropout(pooler)\n        output = self.classifier(pooler)\n        return output\n\nmodel = DistilBERTClass()\nmodel.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    train(epoch)","metadata":{},"execution_count":null,"outputs":[]}]}
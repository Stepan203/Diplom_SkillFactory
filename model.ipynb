{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.024202,"end_time":"2022-06-25T21:33:23.357097","exception":false,"start_time":"2022-06-25T21:33:23.332895","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-26T13:32:19.221241Z","iopub.execute_input":"2022-06-26T13:32:19.222397Z","iopub.status.idle":"2022-06-26T13:32:19.254073Z","shell.execute_reply.started":"2022-06-26T13:32:19.222284Z","shell.execute_reply":"2022-06-26T13:32:19.253205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pymorphy2","metadata":{"papermill":{"duration":12.700286,"end_time":"2022-06-25T21:33:36.062922","exception":false,"start_time":"2022-06-25T21:33:23.362636","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T06:14:22.128477Z","iopub.execute_input":"2022-06-30T06:14:22.128768Z","iopub.status.idle":"2022-06-30T06:14:35.421908Z","shell.execute_reply.started":"2022-06-30T06:14:22.128702Z","shell.execute_reply":"2022-06-30T06:14:35.420773Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nfrom wordcloud import WordCloud\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstopwords_ru = stopwords.words(\"russian\")\nfrom nltk.stem import WordNetLemmatizer\nimport pymorphy2\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n#import transformers\n#import torch\n#import tensorflow as tf\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\n#from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n#from transformers import BertTokenizer, BertModel, BertConfig\n\nimport transformers\nfrom transformers import DistilBertTokenizer\nfrom transformers import DistilBertModel, DistilBertConfig\nfrom transformers import TFDistilBertForSequenceClassification\nimport tensorflow as tf\nimport json\nimport gc","metadata":{"papermill":{"duration":2.007525,"end_time":"2022-06-25T21:33:38.077176","exception":false,"start_time":"2022-06-25T21:33:36.069651","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T06:14:35.425243Z","iopub.execute_input":"2022-06-30T06:14:35.425593Z","iopub.status.idle":"2022-06-30T06:14:43.971502Z","shell.execute_reply.started":"2022-06-30T06:14:35.425566Z","shell.execute_reply":"2022-06-30T06:14:43.970550Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/scan-classification-challange/df_train.csv')\ntrain.sample(3)","metadata":{"papermill":{"duration":0.684804,"end_time":"2022-06-25T21:33:38.768447","exception":false,"start_time":"2022-06-25T21:33:38.083643","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T06:14:43.973073Z","iopub.execute_input":"2022-06-30T06:14:43.973677Z","iopub.status.idle":"2022-06-30T06:14:44.537573Z","shell.execute_reply.started":"2022-06-30T06:14:43.973641Z","shell.execute_reply":"2022-06-30T06:14:44.536684Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Удалим дубликаты\n\ntrain.drop_duplicates(subset={'text'}, inplace=True) \ntrain.shape","metadata":{"papermill":{"duration":0.128957,"end_time":"2022-06-25T21:33:38.975576","exception":false,"start_time":"2022-06-25T21:33:38.846619","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T06:14:44.540465Z","iopub.execute_input":"2022-06-30T06:14:44.540915Z","iopub.status.idle":"2022-06-30T06:14:44.648616Z","shell.execute_reply.started":"2022-06-30T06:14:44.540880Z","shell.execute_reply":"2022-06-30T06:14:44.647534Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# # Названия категорий переводим в числовой формат и записываем в отдельный столбец\n\ntrain['encoded_cat'] = train['class'].astype('category').cat.codes\ntrain.sample(5)","metadata":{"papermill":{"duration":0.035414,"end_time":"2022-06-25T21:33:40.039851","exception":false,"start_time":"2022-06-25T21:33:40.004437","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T06:14:44.650199Z","iopub.execute_input":"2022-06-30T06:14:44.650644Z","iopub.status.idle":"2022-06-30T06:14:44.671968Z","shell.execute_reply.started":"2022-06-30T06:14:44.650605Z","shell.execute_reply":"2022-06-30T06:14:44.671161Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/scan-classification-challange/df_test.csv', index_col=0)\ntest.sample(3)","metadata":{"papermill":{"duration":0.374147,"end_time":"2022-06-25T21:33:40.462568","exception":false,"start_time":"2022-06-25T21:33:40.088421","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T06:14:44.673172Z","iopub.execute_input":"2022-06-30T06:14:44.673492Z","iopub.status.idle":"2022-06-30T06:14:45.020000Z","shell.execute_reply.started":"2022-06-30T06:14:44.673459Z","shell.execute_reply":"2022-06-30T06:14:45.019073Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Посмотрим на состав имеющихся стоп-слов\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstopwords_ru = stopwords.words(\"russian\")\nprint(stopwords.words(\"russian\"))","metadata":{"papermill":{"duration":0.019642,"end_time":"2022-06-25T21:33:40.623466","exception":false,"start_time":"2022-06-25T21:33:40.603824","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T06:14:45.021756Z","iopub.execute_input":"2022-06-30T06:14:45.022418Z","iopub.status.idle":"2022-06-30T06:14:45.030288Z","shell.execute_reply.started":"2022-06-30T06:14:45.022381Z","shell.execute_reply":"2022-06-30T06:14:45.028731Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Теперь выведем все слова длиной менее 3-х символов и дополним список стоп-слов\n# пробовал искать среди стопслов равных 4-м, но там получаем слова 'газа', 'прав', 'дела', 'Рост'\n# как мне кажется данные слова, могут быть полезны для score\nstopworlds_new = set()\nmas_stop = set()\nfor words in train['text']:\n    for i in words.split():\n        if len(i) <= 3:\n            mas_stop.add(i)\nstopworlds_new = set(stopwords_ru).union(mas_stop)","metadata":{"papermill":{"duration":0.33337,"end_time":"2022-06-25T21:33:40.966026","exception":false,"start_time":"2022-06-25T21:33:40.632656","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T06:14:45.032171Z","iopub.execute_input":"2022-06-30T06:14:45.032472Z","iopub.status.idle":"2022-06-30T06:14:45.391305Z","shell.execute_reply.started":"2022-06-30T06:14:45.032445Z","shell.execute_reply":"2022-06-30T06:14:45.390320Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Создадим функцию по очистке данных. Будем переводить слова в нижний регистр, \n# удалять стоп слова, удалять числа и раздичные знаки которые не несут смысловой нагрузки. \n# Все слова преобразуем к их первоначалоной форме (Лемматизация)\n\nmorph = pymorphy2.MorphAnalyzer()\npatterns = \"[A-Z|a-z|0-9!#$%&'()*+,./:“″;”<=>?@[\\]^_`{|}~—\\\"\\-•–«»]+\"\n#stops = set(stopwords.words(\"russian\"))\ndef clean(text):\n    text = text.lower()\n    text = re.sub(patterns, ' ', text)\n    tokens = []\n    for token in text.split():\n        if token and token not in stopworlds_new:\n            token = token.strip()\n            token = morph.normal_forms(token)[0]  # Лемматизация\n            #token = stemmer.stem(token) # Стеммизация\n            tokens.append(token)\n    return ' '.join(tokens)","metadata":{"papermill":{"duration":0.242346,"end_time":"2022-06-25T21:33:41.24453","exception":false,"start_time":"2022-06-25T21:33:41.002184","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T06:14:45.392670Z","iopub.execute_input":"2022-06-30T06:14:45.393027Z","iopub.status.idle":"2022-06-30T06:14:45.636234Z","shell.execute_reply.started":"2022-06-30T06:14:45.392992Z","shell.execute_reply":"2022-06-30T06:14:45.635296Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Применим функцию очистки к train и test\n\ntrain['clean'] = train['text'].apply(lambda x: clean(x))\ntest['clean'] = test['text'].apply(lambda x: clean(x))\ntrain[['clean', 'text']]","metadata":{"papermill":{"duration":143.802204,"end_time":"2022-06-25T21:36:05.057019","exception":false,"start_time":"2022-06-25T21:33:41.254815","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-30T06:14:45.639297Z","iopub.execute_input":"2022-06-30T06:14:45.639748Z","iopub.status.idle":"2022-06-30T06:20:25.551047Z","shell.execute_reply.started":"2022-06-30T06:14:45.639710Z","shell.execute_reply":"2022-06-30T06:20:25.550062Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## FastText","metadata":{}},{"cell_type":"code","source":"ft_train, ft_test = train_test_split(train, random_state=42, test_size=0.2, stratify = train['class'])","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:45.593621Z","iopub.execute_input":"2022-06-26T13:36:45.593956Z","iopub.status.idle":"2022-06-26T13:36:45.659334Z","shell.execute_reply.started":"2022-06-26T13:36:45.593926Z","shell.execute_reply":"2022-06-26T13:36:45.658439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_ft_label(s):\n    return '__label__'+s.replace(',','_').replace(' ','_').replace('-','_')\n\nlabels_dict = {}\nfor g in train['class']:\n    labels_dict[to_ft_label(g)] = g","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:45.660867Z","iopub.execute_input":"2022-06-26T13:36:45.662004Z","iopub.status.idle":"2022-06-26T13:36:45.708407Z","shell.execute_reply.started":"2022-06-26T13:36:45.661959Z","shell.execute_reply":"2022-06-26T13:36:45.707424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import remove_stopwords\ntrain.iloc[:, 0] = train.iloc[:, 0].apply(lambda x: ' '.join(simple_preprocess(x)))\ntest.iloc[:, 1] = test.iloc[:, 1].apply(lambda x: ' '.join(simple_preprocess(x)))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:45.710299Z","iopub.execute_input":"2022-06-26T13:36:45.711134Z","iopub.status.idle":"2022-06-26T13:36:47.999185Z","shell.execute_reply.started":"2022-06-26T13:36:45.711084Z","shell.execute_reply":"2022-06-26T13:36:47.998068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = ['class', 'text']\n\n# train\ntrain_for_ft = ft_train[col]\ntrain_for_ft['class']=[to_ft_label(s) for s in train_for_ft['class']]\n\n# test\ntest_for_ft = ft_test[col]\ntest_for_ft['class']=[to_ft_label(s) for s in test_for_ft['class']]","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:48.000668Z","iopub.execute_input":"2022-06-26T13:36:48.00109Z","iopub.status.idle":"2022-06-26T13:36:48.058371Z","shell.execute_reply.started":"2022-06-26T13:36:48.001054Z","shell.execute_reply":"2022-06-26T13:36:48.056437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_for_ft.to_csv('train_for_ft.csv', index=False, sep=' ', header=False, escapechar=\" \")\ntest_for_ft.to_csv('test_for_ft.csv', index=False, sep=' ', header=False, escapechar=\" \")","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:48.0598Z","iopub.execute_input":"2022-06-26T13:36:48.060158Z","iopub.status.idle":"2022-06-26T13:36:48.420534Z","shell.execute_reply.started":"2022-06-26T13:36:48.060125Z","shell.execute_reply":"2022-06-26T13:36:48.419765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import fasttext\nmodel = fasttext.train_supervised('train_for_ft.csv', lr = 0.9)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:48.421701Z","iopub.execute_input":"2022-06-26T13:36:48.422323Z","iopub.status.idle":"2022-06-26T13:36:51.020585Z","shell.execute_reply.started":"2022-06-26T13:36:48.422287Z","shell.execute_reply":"2022-06-26T13:36:51.019098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.test('test_for_ft.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:51.0218Z","iopub.execute_input":"2022-06-26T13:36:51.022122Z","iopub.status.idle":"2022-06-26T13:36:51.219336Z","shell.execute_reply.started":"2022-06-26T13:36:51.022093Z","shell.execute_reply":"2022-06-26T13:36:51.218144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(test):\n    return labels_dict[ model.predict(test['text'], k=1)[0][0] ]\ntest['predictions'] = test.apply(predict,axis=1)\n\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:51.220781Z","iopub.execute_input":"2022-06-26T13:36:51.221702Z","iopub.status.idle":"2022-06-26T13:36:52.59803Z","shell.execute_reply.started":"2022-06-26T13:36:51.221663Z","shell.execute_reply":"2022-06-26T13:36:52.596864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'id':range(0, len(test)),\n                           'class':test['predictions'].values},\n                          columns=['id', 'class'])\nsubmission.to_csv('submission1.csv', index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:52.599332Z","iopub.execute_input":"2022-06-26T13:36:52.599686Z","iopub.status.idle":"2022-06-26T13:36:52.658191Z","shell.execute_reply.started":"2022-06-26T13:36:52.599652Z","shell.execute_reply":"2022-06-26T13:36:52.656834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sub_predict = model.predict([text_sub_sequences, X_sub])\n#sample_submission['class'] = sub_predict_nn2[:,0]\n#sample_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:36:52.663526Z","iopub.execute_input":"2022-06-26T13:36:52.663928Z","iopub.status.idle":"2022-06-26T13:36:52.668714Z","shell.execute_reply.started":"2022-06-26T13:36:52.663893Z","shell.execute_reply":"2022-06-26T13:36:52.667629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model sklearn\nLogistic Regression","metadata":{}},{"cell_type":"code","source":"# Выделим X, y. X - это будет, наш обработанный текст, y -  наш класс\n\ny = train.encoded_cat.values\nX = train.drop(['encoded_cat', 'text'], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-30T06:20:25.553122Z","iopub.execute_input":"2022-06-30T06:20:25.553724Z","iopub.status.idle":"2022-06-30T06:20:25.561427Z","shell.execute_reply.started":"2022-06-30T06:20:25.553687Z","shell.execute_reply":"2022-06-30T06:20:25.560285Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"## Разделим все данные на train test в соотношении 80/20\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T06:20:25.563201Z","iopub.execute_input":"2022-06-30T06:20:25.563621Z","iopub.status.idle":"2022-06-30T06:20:25.603361Z","shell.execute_reply.started":"2022-06-30T06:20:25.563581Z","shell.execute_reply":"2022-06-30T06:20:25.602372Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Необходимо преобразовать тест в токены, сделаем несколькими способами и посмотрим на результат","metadata":{}},{"cell_type":"code","source":"## CountVectorizer преобразует текст в матрицу количества токенов\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer()\n\nX_train_review_bow = vect.fit_transform(X_train['clean'])\nX_test_review_bow = vect.transform(X_test['clean'])\nX_sub_rev_bow = vect.transform(test['clean'])\n#X_train_review_bow = vect.fit_transform(X_train)\n#X_test_review_bow = vect.transform(X_test)\n\nprint('X_train_review_bow shape: ', X_train_review_bow.shape)\nprint('X_test_review_bow shape: ', X_test_review_bow.shape)\nprint('X_sub_review_bow shape: ', X_sub_rev_bow.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T06:20:25.605942Z","iopub.execute_input":"2022-06-30T06:20:25.606698Z","iopub.status.idle":"2022-06-30T06:20:27.630230Z","shell.execute_reply.started":"2022-06-30T06:20:25.606661Z","shell.execute_reply":"2022-06-30T06:20:27.629114Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"## Tf-Idf преобразует текст в матрицу функций TF-IDF (частота обратная частоте документа)\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\nX_train_review_tfidf = vectorizer.fit_transform(X_train['clean'])\nX_test_review_tfidf = vectorizer.transform(X_test['clean'])\nX_sub_review_tfidf = vectorizer.transform(test['clean'])\n\nprint('X_train_review_tfidf shape: ', X_train_review_tfidf.shape)\nprint('X_test_review_tfidf shape: ', X_test_review_tfidf.shape)\nprint('X_sub_review_tfidf shape: ', X_sub_review_tfidf.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T06:20:27.631649Z","iopub.execute_input":"2022-06-30T06:20:27.632592Z","iopub.status.idle":"2022-06-30T06:20:29.519610Z","shell.execute_reply.started":"2022-06-30T06:20:27.632550Z","shell.execute_reply":"2022-06-30T06:20:29.518458Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Построим модель логистической регресии, в качестве метрики будем выводить accuracy и F1\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\n\nclf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', random_state=42)\nclf.fit(X_train_review_tfidf, y_train)\n\n#y_pred = clf.predict(X_test_review_tfidf)\ny_pred = clf.predict(X_test_review_tfidf)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T06:20:29.521264Z","iopub.execute_input":"2022-06-30T06:20:29.521644Z","iopub.status.idle":"2022-06-30T06:21:33.833664Z","shell.execute_reply.started":"2022-06-30T06:20:29.521606Z","shell.execute_reply":"2022-06-30T06:21:33.832619Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T06:21:33.835189Z","iopub.execute_input":"2022-06-30T06:21:33.835789Z","iopub.status.idle":"2022-06-30T06:22:37.376496Z","shell.execute_reply.started":"2022-06-30T06:21:33.835750Z","shell.execute_reply":"2022-06-30T06:22:37.375468Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Попробуем другие модели sklearn","metadata":{}},{"cell_type":"code","source":"# C-классификация опорных векторов\n\nfrom sklearn.svm import SVC\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train_review_bow, y_train)\ny_pred_svc = svclassifier.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred_svc))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T06:22:37.378549Z","iopub.execute_input":"2022-06-30T06:22:37.379453Z","iopub.status.idle":"2022-06-30T06:26:32.764278Z","shell.execute_reply.started":"2022-06-30T06:22:37.379409Z","shell.execute_reply":"2022-06-30T06:26:32.763184Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Классификатор, реализующий голосование k ближайших соседей\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 10)\nknn.fit(X_train_review_bow, y_train)\ny_pred = knn.predict(X_test_review_bow)\n\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T06:26:32.765836Z","iopub.execute_input":"2022-06-30T06:26:32.766216Z","iopub.status.idle":"2022-06-30T06:26:47.231462Z","shell.execute_reply.started":"2022-06-30T06:26:32.766181Z","shell.execute_reply":"2022-06-30T06:26:47.230425Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Гипер параметры","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n\"\"\"model = LogisticRegression(multi_class='multinomial', penalty='l2', random_state=42)\n#penalty = ['l1','l2']\nsolver = ['newton-cg', 'sag', 'saga', 'lbfgs']\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\nalpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\ngrid = dict(alpha=alpha)\nc_values = [100, 10, 1.0]\n#param_grid = {'C':[1, 10]}\nparam_grid = dict(C=c_values, solver=solver)\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, verbose=10, cv=cv, scoring='accuracy', error_score=0)\ngrid_search.fit(X_train_review_bow, y_train)\n#print(\"Best: %f using %s\" % (grid_search.best_params_))\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-30T06:34:20.960537Z","iopub.execute_input":"2022-06-30T06:34:20.960901Z","iopub.status.idle":"2022-06-30T06:34:20.969802Z","shell.execute_reply.started":"2022-06-30T06:34:20.960871Z","shell.execute_reply":"2022-06-30T06:34:20.968694Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='newton-cg', penalty='l2', C = 10, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T06:34:26.746268Z","iopub.execute_input":"2022-06-30T06:34:26.746715Z","iopub.status.idle":"2022-06-30T06:36:41.462422Z","shell.execute_reply.started":"2022-06-30T06:34:26.746676Z","shell.execute_reply":"2022-06-30T06:36:41.461211Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(multi_class='multinomial', solver='saga', penalty='l2', C = 100, random_state=42)\nclf.fit(X_train_review_bow, y_train)\n\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T06:36:41.465142Z","iopub.execute_input":"2022-06-30T06:36:41.465514Z","iopub.status.idle":"2022-06-30T06:37:06.846795Z","shell.execute_reply.started":"2022-06-30T06:36:41.465475Z","shell.execute_reply":"2022-06-30T06:37:06.845744Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Стекинг","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\n#from sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\n\n#data, target = load_breast_cancer(return_X_y=True)\n\nestimators = [('lr', LogisticRegression()), ('svc', SVC(kernel='linear'))]\nmodelClf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n\n#X_train, X_valid, y_train, y_valid = train_test_split(data, target, test_size=0.3, random_state=12)\n\nmodelClf.fit(X_train_review_bow, y_train)\ny_pred = clf.predict(X_test_review_bow)\nprint('Test Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Test F1: ', f1_score(y_test, y_pred, average='weighted'))\n#print(modelClf.score(X_valid, y_valid))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T06:37:06.848192Z","iopub.execute_input":"2022-06-30T06:37:06.849159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bert TenzorFlow","metadata":{}},{"cell_type":"code","source":"!pip install transformers\n!pip install pytorch\n!pip install pytorch-transformers","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:19:44.586282Z","iopub.execute_input":"2022-06-29T11:19:44.586613Z","iopub.status.idle":"2022-06-29T11:20:14.817744Z","shell.execute_reply.started":"2022-06-29T11:19:44.586577Z","shell.execute_reply":"2022-06-29T11:20:14.816576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import DistilBertTokenizer\nfrom transformers import DistilBertModel, DistilBertConfig\nfrom transformers import TFDistilBertForSequenceClassification\nimport tensorflow as tf\nimport json\nimport gc","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:20:14.819591Z","iopub.execute_input":"2022-06-29T11:20:14.819888Z","iopub.status.idle":"2022-06-29T11:20:14.825807Z","shell.execute_reply.started":"2022-06-29T11:20:14.819859Z","shell.execute_reply":"2022-06-29T11:20:14.82464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Токенизируем текст \n\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nX_train_encodings = tokenizer(X_train['clean'].to_list(), truncation=True, padding=True)\ntest_encodings = tokenizer(test['text'].to_list(), truncation=True, padding=True)\n\nX_test_encodings = tokenizer(X_test['clean'].to_list(), truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T16:46:47.678826Z","iopub.execute_input":"2022-06-27T16:46:47.679627Z","iopub.status.idle":"2022-06-27T16:46:51.272714Z","shell.execute_reply.started":"2022-06-27T16:46:47.679588Z","shell.execute_reply":"2022-06-27T16:46:51.270632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создает dataset с тонекизированными данными\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(X_train_encodings),\n    y_train\n))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(X_test_encodings), \n                                    list(y_test))) \nval_class = np.zeros(len(test))\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(test_encodings),\n    val_class))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:52:08.614508Z","iopub.execute_input":"2022-06-26T17:52:08.614849Z","iopub.status.idle":"2022-06-26T17:55:27.22069Z","shell.execute_reply.started":"2022-06-26T17:52:08.614816Z","shell.execute_reply":"2022-06-26T17:55:27.219706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class F1_Score(tf.keras.metrics.Metric):\n\n    def __init__(self, name='f1_score', **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.f1 = self.add_weight(name='f1', initializer='zeros')\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        p = Precision(thresholds=0.5)(y_true, y_pred)\n        r = Recall(thresholds=0.5)(y_true, y_pred)\n        self.f1 = 2 * ((p * r) / (p + r + 1e-6))\n\n    def result(self):\n        return self.f1\n\n    def reset_states(self):\n        self.f1.assign(0)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:15:02.754652Z","iopub.execute_input":"2022-06-26T17:15:02.756053Z","iopub.status.idle":"2022-06-26T17:15:02.766457Z","shell.execute_reply.started":"2022-06-26T17:15:02.756002Z","shell.execute_reply":"2022-06-26T17:15:02.76524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\n\ndef recall_m(y_train, y_test):\n    true_positives = K.sum(K.round(K.clip(y_train * y_test, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_train, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_train, y_test):\n    true_positives = K.sum(K.round(K.clip(y_train * y_test, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_test, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_train, y_test):\n    precision = precision_m(y_train, y_test)\n    recall = recall_m(y_train, y_test)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:55:27.222123Z","iopub.execute_input":"2022-06-26T17:55:27.222721Z","iopub.status.idle":"2022-06-26T17:55:27.232078Z","shell.execute_reply.started":"2022-06-26T17:55:27.222682Z","shell.execute_reply":"2022-06-26T17:55:27.2308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=50)\nlosss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) # 2e-5\nmodel.compile(optimizer=optimizer, loss=model.hf_compute_loss(), metrics=['accuracy', f1_m]) # loss=model.compute_loss","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:55:27.233501Z","iopub.execute_input":"2022-06-26T17:55:27.23392Z","iopub.status.idle":"2022-06-26T17:55:53.441807Z","shell.execute_reply.started":"2022-06-26T17:55:27.233884Z","shell.execute_reply":"2022-06-26T17:55:53.440946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_dataset.shuffle(1000).batch(16), \n          epochs=10,\n          validation_data=test_dataset.shuffle(1000).batch(16))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:55:53.444834Z","iopub.execute_input":"2022-06-26T17:55:53.445151Z","iopub.status.idle":"2022-06-26T20:56:52.332964Z","shell.execute_reply.started":"2022-06-26T17:55:53.445108Z","shell.execute_reply":"2022-06-26T20:56:52.332059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distill-Bert fine-tuning - Huggingface and Pytorch","metadata":{}},{"cell_type":"code","source":"#!pip install datasets\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id2label = {str(i): label for i, label in enumerate(train[\"class\"].unique().tolist())}\nlabel2id = {v: k for k, v in id2label.items()}\nprint(label2id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Токенизатор DistilBERT будет принимать обучающие данные только в том случае, если у него есть labels столбец, \n# поэтому мы добавим еще один столбец с нашими сопоставленными метками.\n\ntrain = (train.assign(labels=train[\"class\"].map(label2id)))\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создаем Dataset и разбиваем на test и train\n\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\n\ndataset = Dataset.from_pandas(train).train_test_split(train_size=0.8, seed=123)\nprint(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Затем мы создаем Datasetиз фрейма данных и разбиваем его на a testи trainустанавливаем.\n\ndataset = dataset.class_encode_column(\"labels\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# загружаем трансформатор AutoTokenizer для DistilBERT\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Составляем список столбцов, которые нужно удалить из набора данных при токенизации\n\ncols_to_remove = [col for col in dataset[\"train\"].column_names if col != \"labels\"]\nprint(cols_to_remove)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# После завершения токенизации мы подготавливаем набор данных для передачи в модель, устанавливая его формат \"torch\"\n\ndef tokenize(batch):\n    tokenized_batch = tokenizer(batch['text'], padding=True, truncation=True, max_length=128)\n    tokenized_batch['labels'] = [int(label) for label in batch[\"labels\"]]\n    return tokenized_batch\n\ndataset_enc = dataset.map(tokenize, batched=True, remove_columns=cols_to_remove, num_proc=4)\ndataset_enc.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n\nprint(dataset_enc[\"train\"].column_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создаем экземпляр a DataLoader для каждого разделения набора данных, чтобы передать его модели\n\nfrom transformers import DataCollatorWithPadding\nfrom torch.utils.data import DataLoader\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrain_dataloader = DataLoader(\n    dataset_enc[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n)\neval_dataloader = DataLoader(\n    dataset_enc[\"test\"], batch_size=8, collate_fn=data_collator\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Загружаем модель\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AutoModelForSequenceClassification\n\n# Load model from checkpoint\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", \n                                                           num_labels=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Определяем гтперпараметры, оптимизатор, скорость, планировщик\n\nfrom transformers import AdamW\nfrom transformers import get_scheduler\n\nlearning_rate = 5e-5\nnum_epochs = 3\n\noptimizer = AdamW(model.parameters(), lr=learning_rate)\n\nnum_training_batches = len(train_dataloader)\nnum_training_steps = num_epochs * num_training_batches\nlr_scheduler = get_scheduler(\n    \"linear\",                   \n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Перемещаем модель на GPU\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Запускаем обучение модели\n\nfrom tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(num_training_steps))\n\n# Train the model with PyTorch training loop\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Оценка модели\n\nfrom datasets import load_metric\n\n# Load metric\nmetric = load_metric(\"glue\", \"mrpc\")\n\n# Iteratively evaluate the model and compute metrics\nmodel.eval()\nfor batch in eval_dataloader:\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**batch)\n\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n\n# Get model accuracy and F1 score\nmetric.compute(predictions=predictions, references=batch[\"labels\"], average=\"weighted\")\n#metric.compute(predictions=predictions, references=batch[\"labels\"],average=\"weighted\")","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:46:19.37353Z","iopub.execute_input":"2022-06-29T22:46:19.37452Z","iopub.status.idle":"2022-06-29T22:46:31.922056Z","shell.execute_reply.started":"2022-06-29T22:46:19.374399Z","shell.execute_reply":"2022-06-29T22:46:31.920277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bert PyTorch","metadata":{}},{"cell_type":"code","source":"# Создадим словарь из названия класса и его кодированног числа\nlabels = train.set_index('class').to_dict()['encoded_cat']","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:14:54.8539Z","iopub.execute_input":"2022-06-29T17:14:54.854383Z","iopub.status.idle":"2022-06-29T17:14:55.067433Z","shell.execute_reply.started":"2022-06-29T17:14:54.854333Z","shell.execute_reply":"2022-06-29T17:14:55.066535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\n\n#tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased'\n#num_labels=50\n# do_lower_case=True\n)\nexample_text = 'I will watch Memento tonight'\nbert_input = tokenizer(example_text,padding='max_length', max_length = 10, \n                       truncation=True, return_tensors=\"pt\")\n\n\nprint(bert_input['input_ids'])\nprint(bert_input['token_type_ids'])\nprint(bert_input['attention_mask'])","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:15:10.556839Z","iopub.execute_input":"2022-06-29T17:15:10.557219Z","iopub.status.idle":"2022-06-29T17:15:11.980424Z","shell.execute_reply.started":"2022-06-29T17:15:10.557186Z","shell.execute_reply":"2022-06-29T17:15:11.978484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n\nclass Dataset(torch.utils.data.Dataset):\n\n    def __init__(self, df):\n\n        self.labels = [labels[label] for label in df['class']]\n        self.texts = [tokenizer(text, \n                               padding='max_length', max_length = 512, truncation=True,\n                                return_tensors=\"pt\") for text in df['text']]\n\n    def classes(self):\n        return self.labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def get_batch_labels(self, idx):\n        # Fetch a batch of labels\n        return np.array(self.labels[idx])\n       # return torch.LongTensor (self.labels[idx])\n\n    def get_batch_texts(self, idx):\n        # Fetch a batch of inputs\n        return self.texts[idx]\n\n    def __getitem__(self, idx):\n\n        batch_texts = self.get_batch_texts(idx)\n        batch_y = self.get_batch_labels(idx)\n\n        return batch_texts, batch_y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Раздели данные текста на тестовую и тренировочную части\n\nnp.random.seed(112)\ndf_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n                                     [int(.8*len(df)), int(.9*len(df))])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Создадим 2 переменные, первая переменная, которую мы назвали _ в приведенном ниже коде, содержит векторы встраивания всех токенов в последовательности. Вторая переменная, которую мы назвали pooled_output, содержит вектор встраивания токена [CLS] Затем мы передаем pooled_output переменную в линейный слой с функцией активации ReLU. В конце линейного слоя у нас есть вектор размера 50, каждый из которых соответствует категории наших классов","metadata":{}},{"cell_type":"code","source":"from torch import nn\nfrom transformers import BertModel\n\nclass BertClassifier(nn.Module):\n\n    def __init__(self, dropout=0.5):\n\n        super(BertClassifier, self).__init__()\n\n        self.bert = BertModel.from_pretrained('bert-base-cased')\n        self.dropout = nn.Dropout(dropout)\n        self.linear = nn.Linear(768, 50)\n        self.relu = nn.ReLU()\n\n    def forward(self, input_id, mask):\n\n        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n        dropout_output = self.dropout(pooled_output)\n        linear_output = self.linear(dropout_output)\n        final_layer = self.relu(linear_output)\n\n        return final_layer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Тренировочный цикл","metadata":{}},{"cell_type":"code","source":"from torch.optim import Adam\nfrom tqdm import tqdm\n\ndef train(model, train_data, val_data, learning_rate, epochs):\n\n    train, val = Dataset(train_data), Dataset(val_data)\n\n    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = Adam(model.parameters(), lr= learning_rate)\n\n    if use_cuda:\n\n            model = model.cuda()\n            criterion = criterion.cuda()\n\n    for epoch_num in range(epochs):\n\n            total_acc_train = 0\n            total_loss_train = 0\n\n            for train_input, train_label in tqdm(train_dataloader):\n\n                train_label = train_label.to(device)\n                mask = train_input['attention_mask'].to(device)\n                input_id = train_input['input_ids'].squeeze(1).to(device)\n\n                output = model(input_id, mask)\n                \n                batch_loss = criterion(output, train_label.long())\n                total_loss_train += batch_loss.item()\n                \n                acc = (output.argmax(dim=1) == train_label).sum().item()\n                total_acc_train += acc\n\n                model.zero_grad()\n                batch_loss.backward()\n                optimizer.step()\n            \n            total_acc_val = 0\n            total_loss_val = 0\n\n            with torch.no_grad():\n\n                for val_input, val_label in val_dataloader:\n\n                    val_label = val_label.to(device)\n                    mask = val_input['attention_mask'].to(device)\n                    input_id = val_input['input_ids'].squeeze(1).to(device)\n\n                    output = model(input_id, mask)\n\n                    batch_loss = criterion(output, val_label.long())\n                    total_loss_val += batch_loss.item()\n                    \n                    acc = (output.argmax(dim=1) == val_label).sum().item()\n                    total_acc_val += acc\n            \n            print(\n                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n                  \nEPOCHS = 5\nmodel = BertClassifier()\nLR = 1e-6\n              \ntrain(model, df_train, df_val, LR, EPOCHS)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
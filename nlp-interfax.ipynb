{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-12T08:12:20.022558Z","iopub.execute_input":"2022-06-12T08:12:20.022961Z","iopub.status.idle":"2022-06-12T08:12:20.032372Z","shell.execute_reply.started":"2022-06-12T08:12:20.022929Z","shell.execute_reply":"2022-06-12T08:12:20.03114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pymorphy2","metadata":{"execution":{"iopub.status.busy":"2022-06-12T23:31:49.133778Z","iopub.execute_input":"2022-06-12T23:31:49.134179Z","iopub.status.idle":"2022-06-12T23:32:03.993036Z","shell.execute_reply.started":"2022-06-12T23:31:49.134147Z","shell.execute_reply":"2022-06-12T23:32:03.992063Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport sys\nimport PIL\nimport cv2\nimport re\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# # keras\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport albumentations\n\n# plt\nimport matplotlib.pyplot as plt\n#увеличим дефолтный размер графиков\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\n#графики в svg выглядят более четкими\n%config InlineBackend.figure_format = 'svg' \n%matplotlib inline\n","metadata":{"execution":{"iopub.status.busy":"2022-06-12T23:28:40.228096Z","iopub.execute_input":"2022-06-12T23:28:40.228460Z","iopub.status.idle":"2022-06-12T23:28:48.351562Z","shell.execute_reply.started":"2022-06-12T23:28:40.228430Z","shell.execute_reply":"2022-06-12T23:28:48.350830Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# всегда фиксируйте RANDOM_SEED, чтобы ваши эксперименты были воспроизводимы!\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T23:30:21.299754Z","iopub.execute_input":"2022-06-12T23:30:21.300110Z","iopub.status.idle":"2022-06-12T23:30:21.304607Z","shell.execute_reply.started":"2022-06-12T23:30:21.300081Z","shell.execute_reply":"2022-06-12T23:30:21.303601Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/scan-classification-challange/df_train.csv')\ntrain.sample(3)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T23:30:25.315176Z","iopub.execute_input":"2022-06-12T23:30:25.315569Z","iopub.status.idle":"2022-06-12T23:30:25.925946Z","shell.execute_reply.started":"2022-06-12T23:30:25.315535Z","shell.execute_reply":"2022-06-12T23:30:25.925066Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:38:19.285180Z","iopub.execute_input":"2022-06-12T18:38:19.285751Z","iopub.status.idle":"2022-06-12T18:38:19.332776Z","shell.execute_reply.started":"2022-06-12T18:38:19.285702Z","shell.execute_reply":"2022-06-12T18:38:19.330793Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/scan-classification-challange/df_test.csv')\ntest.sample(3)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T23:30:29.436017Z","iopub.execute_input":"2022-06-12T23:30:29.436414Z","iopub.status.idle":"2022-06-12T23:30:29.810861Z","shell.execute_reply.started":"2022-06-12T23:30:29.436381Z","shell.execute_reply":"2022-06-12T23:30:29.809801Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"test.drop(['Unnamed: 0'], inplace=True, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:38:25.833877Z","iopub.execute_input":"2022-06-12T18:38:25.834810Z","iopub.status.idle":"2022-06-12T18:38:25.841645Z","shell.execute_reply.started":"2022-06-12T18:38:25.834759Z","shell.execute_reply":"2022-06-12T18:38:25.840734Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/scan-classification-challange/sample_submission.csv')\nsample_submission","metadata":{"execution":{"iopub.status.busy":"2022-06-12T23:30:37.176769Z","iopub.execute_input":"2022-06-12T23:30:37.177144Z","iopub.status.idle":"2022-06-12T23:30:37.200592Z","shell.execute_reply.started":"2022-06-12T23:30:37.177113Z","shell.execute_reply":"2022-06-12T23:30:37.199753Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2022-06-12T23:29:30.846905Z","iopub.execute_input":"2022-06-12T23:29:30.847395Z","iopub.status.idle":"2022-06-12T23:29:31.111783Z","shell.execute_reply.started":"2022-06-12T23:29:30.847328Z","shell.execute_reply":"2022-06-12T23:29:31.110682Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"Средняя длина, стандартное отклонение, макс.длина столбца text\nlens = train.text.str.len()\nlens.mean(), lens.std(), lens.max()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:38:31.221484Z","iopub.execute_input":"2022-06-12T18:38:31.221866Z","iopub.status.idle":"2022-06-12T18:38:31.282792Z","shell.execute_reply.started":"2022-06-12T18:38:31.221834Z","shell.execute_reply":"2022-06-12T18:38:31.281666Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"lens.hist();","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:38:47.694139Z","iopub.execute_input":"2022-06-12T18:38:47.694565Z","iopub.status.idle":"2022-06-12T18:38:47.979540Z","shell.execute_reply.started":"2022-06-12T18:38:47.694532Z","shell.execute_reply":"2022-06-12T18:38:47.978445Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"pd.unique(train['class'])","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:41:41.877819Z","iopub.execute_input":"2022-06-12T18:41:41.878225Z","iopub.status.idle":"2022-06-12T18:41:41.890375Z","shell.execute_reply.started":"2022-06-12T18:41:41.878194Z","shell.execute_reply":"2022-06-12T18:41:41.889235Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train['sample'] = 1 # помечаем где у нас трейн\ntest['sample'] = 0 # помечаем где у нас тест\n#test['class'] = 0 # в тесте у нас нет значения price, мы его должны предсказать, поэтому пока просто заполняем нулями\n\ndf = test.append(train, sort=False).reset_index(drop=True) # объединяем\nprint(train.shape, test.shape, df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:42:43.775350Z","iopub.execute_input":"2022-06-12T18:42:43.775857Z","iopub.status.idle":"2022-06-12T18:42:43.797126Z","shell.execute_reply.started":"2022-06-12T18:42:43.775822Z","shell.execute_reply":"2022-06-12T18:42:43.795315Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Смотрим кол-во по каждому классу\ntrain['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:43:19.175080Z","iopub.execute_input":"2022-06-12T18:43:19.175483Z","iopub.status.idle":"2022-06-12T18:43:19.191106Z","shell.execute_reply.started":"2022-06-12T18:43:19.175452Z","shell.execute_reply":"2022-06-12T18:43:19.190355Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# создаем новый столбец, закодирует столбец с классами\ndf['encoded_cat'] = df['class'].astype('category').cat.codes\ndf.drop(['class'], inplace=True, axis=1)\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:44:20.562820Z","iopub.execute_input":"2022-06-12T18:44:20.563233Z","iopub.status.idle":"2022-06-12T18:44:20.593607Z","shell.execute_reply.started":"2022-06-12T18:44:20.563200Z","shell.execute_reply":"2022-06-12T18:44:20.592573Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Stopworlds","metadata":{}},{"cell_type":"code","source":"# Посмотрим на состав имеющихся стоп-слов\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstopwords_ru = stopwords.words(\"russian\")\nprint(stopwords.words(\"russian\"))","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:44:27.767959Z","iopub.execute_input":"2022-06-12T18:44:27.768343Z","iopub.status.idle":"2022-06-12T18:44:29.266938Z","shell.execute_reply.started":"2022-06-12T18:44:27.768311Z","shell.execute_reply":"2022-06-12T18:44:29.265946Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Теперь выведем все слова длиной менее 3-х символов и дополним список стоп-слов\n\nstopworlds_new = set()\nmas_stop = set()\nfor words in df['text']:\n    for i in words.split():\n        if len(i) <= 3:\n            mas_stop.add(i)\nprint(mas_stop)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T19:32:27.082055Z","iopub.execute_input":"2022-06-12T19:32:27.082671Z","iopub.status.idle":"2022-06-12T19:32:27.746024Z","shell.execute_reply.started":"2022-06-12T19:32:27.082638Z","shell.execute_reply":"2022-06-12T19:32:27.744857Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# Создаем новый список стоп-слов\nstopworlds_new = set(stopwords_ru).union(mas_stop)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T19:32:57.421462Z","iopub.execute_input":"2022-06-12T19:32:57.421891Z","iopub.status.idle":"2022-06-12T19:32:57.427816Z","shell.execute_reply.started":"2022-06-12T19:32:57.421857Z","shell.execute_reply":"2022-06-12T19:32:57.426713Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# Создадим функцию, где будем удалять удалять различные знаки пунктуации, цифры и прочие символы\n# Также будем исключать стоп слова, приводить слова к нижнему регистру\n#stemmer = SnowballStemmer(\"russian\") \nimport pymorphy2\nmorph = pymorphy2.MorphAnalyzer()\n\npatterns = \"[A-Z|a-z|0-9!#$%&'()*+,./:“″;”<=>?@[\\]^_`{|}~—\\\"\\-•–«»]+\"\ndef lemmatize(doc):\n    doc = doc.lower()\n    doc = re.sub(patterns, ' ', doc)\n    tokens = []\n    for token in doc.split():\n        if token and token not in stopworlds_new:\n            token = token.strip()\n            token = morph.normal_forms(token)[0]  # Лемматизация\n            #token = stemmer.stem(token) # Стеммизация\n            tokens.append(token)\n #   if len(tokens) >=0 :\n     #   tokens = [word for word in tokens if not word in worlds_freq]\n    return ' '.join(tokens)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T19:40:54.372616Z","iopub.execute_input":"2022-06-12T19:40:54.373746Z","iopub.status.idle":"2022-06-12T19:40:54.696315Z","shell.execute_reply.started":"2022-06-12T19:40:54.373704Z","shell.execute_reply":"2022-06-12T19:40:54.695525Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"df['text_clean'] = df.text.apply(lambda x: lemmatize(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-12T19:42:26.888628Z","iopub.execute_input":"2022-06-12T19:42:26.889004Z","iopub.status.idle":"2022-06-12T19:48:56.673347Z","shell.execute_reply.started":"2022-06-12T19:42:26.888973Z","shell.execute_reply":"2022-06-12T19:48:56.672079Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# Выведем список слов которые часто встречаются\nwords = list( df['text_clean'].values)\n\nsplit_words=[]\nfor word in words:\n    lo_w=[]\n    list_of_words=str(word).split()\n    for w in list_of_words:\n        if w not in stopworlds_new:\n            lo_w.append(w)\n    split_words.append(lo_w)\nallwords = []\nfor wordlist in split_words:\n    allwords += wordlist","metadata":{"execution":{"iopub.status.busy":"2022-06-12T19:53:33.153622Z","iopub.execute_input":"2022-06-12T19:53:33.154627Z","iopub.status.idle":"2022-06-12T19:53:33.937295Z","shell.execute_reply.started":"2022-06-12T19:53:33.154572Z","shell.execute_reply":"2022-06-12T19:53:33.936427Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"# Получим 10 часто встречающихся слов\n\nfrom nltk import FreqDist\nword_freq = FreqDist(allwords).most_common(10)\nword_freq","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:45:58.378779Z","iopub.execute_input":"2022-06-12T20:45:58.379183Z","iopub.status.idle":"2022-06-12T20:45:59.549822Z","shell.execute_reply.started":"2022-06-12T20:45:58.379152Z","shell.execute_reply":"2022-06-12T20:45:59.548861Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"# создадим список из 10 слов наиболее часто встречающихся\nworlds_freq = ['который', 'сообщить', 'дело', 'заявить', 'россия', 'сказать', 'свой', 'рф', 'сообщать', 'также']\n#world_freq = []\n#for i in word_freq:\n#    print(str(i[:1]).strip())\n#    world_freq.append(i[:1])\n#world_freq","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:22:27.043072Z","iopub.execute_input":"2022-06-12T20:22:27.043979Z","iopub.status.idle":"2022-06-12T20:22:27.048997Z","shell.execute_reply.started":"2022-06-12T20:22:27.043944Z","shell.execute_reply":"2022-06-12T20:22:27.047972Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"#patterns = \"[A-Z|a-z|0-9!#$%&'()*+,./:“″;”<=>?@[\\]^_`{|}~—\\\"\\-•–«»]+\"\ndef freq(doc):\n    doc = doc.lower()\n    doc = re.sub(patterns, ' ', doc)\n    tokens = []\n    for token in doc.split():\n        if token and token not in stopworlds_new:\n            token = token.strip()\n            token = morph.normal_forms(token)[0]  # Лемматизация\n           # token = stemmer.stem(token) # Стеммизация\n            tokens.append(token)\n        if len(tokens) >=0 :\n            tokens = [word for word in tokens if not word in worlds_freq] # добавим исключение частовстречающихся слов\n    return ' '.join(tokens)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:33:36.673622Z","iopub.execute_input":"2022-06-12T20:33:36.674246Z","iopub.status.idle":"2022-06-12T20:33:36.680973Z","shell.execute_reply.started":"2022-06-12T20:33:36.674211Z","shell.execute_reply":"2022-06-12T20:33:36.680240Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"df['text_clean_freq'] = df.text_clean.apply(lambda x: freq(x))\ndf.text_clean_freq","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:33:38.192703Z","iopub.execute_input":"2022-06-12T20:33:38.193505Z","iopub.status.idle":"2022-06-12T20:39:16.722580Z","shell.execute_reply.started":"2022-06-12T20:33:38.193464Z","shell.execute_reply":"2022-06-12T20:39:16.721535Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"# world cloud \n# выводим облако слов, сразу применяем стоп слова, и слова к нижнему регистру\ncomment_words = ''\nstopwords = set(STOPWORDS)\n \n# iterate through the csv file\n#for val in df['text']:\n#    val = str(val)\n#    tokens = val.split()\n#for i in range(len(allwords)):\n #   tokens[i] = tokens[i].lower()\n     \ncomment_words += \" \".join(allwords)+\" \" \nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n               # stopwords = stopwords,\n                stopwords = stopworlds_new,\n                min_font_size = 10).generate(comment_words)                     \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:43:33.987009Z","iopub.execute_input":"2022-06-12T20:43:33.987439Z","iopub.status.idle":"2022-06-12T20:43:48.725057Z","shell.execute_reply.started":"2022-06-12T20:43:33.987405Z","shell.execute_reply":"2022-06-12T20:43:48.723954Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"# Теперь выделим тестовую часть\ntrain_data = df.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df.query('sample == 0').drop(['sample'], axis=1)\ny = train_data.encoded_cat.values     # наш таргет\nX = train_data.drop(['text', 'text_clean', 'encoded_cat'], axis=1)\nX_sub = df.drop(['text_clean', 'text_clean_freq', 'sample'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:05:53.064261Z","iopub.execute_input":"2022-06-12T21:05:53.064660Z","iopub.status.idle":"2022-06-12T21:05:53.106192Z","shell.execute_reply.started":"2022-06-12T21:05:53.064629Z","shell.execute_reply":"2022-06-12T21:05:53.105061Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=RANDOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:11:02.120362Z","iopub.execute_input":"2022-06-12T21:11:02.121535Z","iopub.status.idle":"2022-06-12T21:11:02.135405Z","shell.execute_reply.started":"2022-06-12T21:11:02.121485Z","shell.execute_reply":"2022-06-12T21:11:02.134325Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"# split данных\ntext_train = df.text_clean_freq.iloc[X_train.index]\ntext_test = df.text_clean_freq.iloc[X_test.index]\ntext_sub = df.text_clean_freq.iloc[X_sub.index]","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:13:51.895105Z","iopub.execute_input":"2022-06-12T21:13:51.895487Z","iopub.status.idle":"2022-06-12T21:13:51.911794Z","shell.execute_reply.started":"2022-06-12T21:13:51.895456Z","shell.execute_reply":"2022-06-12T21:13:51.911056Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"# TOKENIZER\n# The maximum number of words to be used. (most frequent)\nMAX_WORDS = 100000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 256","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:19:10.931755Z","iopub.execute_input":"2022-06-12T21:19:10.932746Z","iopub.status.idle":"2022-06-12T21:19:10.937279Z","shell.execute_reply.started":"2022-06-12T21:19:10.932704Z","shell.execute_reply":"2022-06-12T21:19:10.936432Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"%%time\ntokenize = Tokenizer(num_words=MAX_WORDS)\ntokenize.fit_on_texts(df.text_clean_freq)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:19:40.627277Z","iopub.execute_input":"2022-06-12T21:19:40.627891Z","iopub.status.idle":"2022-06-12T21:19:43.971042Z","shell.execute_reply.started":"2022-06-12T21:19:40.627775Z","shell.execute_reply":"2022-06-12T21:19:43.969872Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"%%time\ntext_train_sequences = sequence.pad_sequences(tokenize.texts_to_sequences(text_train), maxlen=MAX_SEQUENCE_LENGTH)\ntext_test_sequences = sequence.pad_sequences(tokenize.texts_to_sequences(text_test), maxlen=MAX_SEQUENCE_LENGTH)\ntext_sub_sequences = sequence.pad_sequences(tokenize.texts_to_sequences(text_sub), maxlen=MAX_SEQUENCE_LENGTH)\n\nprint(text_train_sequences.shape, text_test_sequences.shape, text_sub_sequences.shape, )","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:19:51.654196Z","iopub.execute_input":"2022-06-12T21:19:51.654597Z","iopub.status.idle":"2022-06-12T21:19:56.859919Z","shell.execute_reply.started":"2022-06-12T21:19:51.654564Z","shell.execute_reply":"2022-06-12T21:19:56.858876Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"# вот так теперь выглядит наш текст\nprint(text_train.iloc[6])\nprint(text_train_sequences[6])","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:20:00.924243Z","iopub.execute_input":"2022-06-12T21:20:00.924681Z","iopub.status.idle":"2022-06-12T21:20:00.933935Z","shell.execute_reply.started":"2022-06-12T21:20:00.924647Z","shell.execute_reply":"2022-06-12T21:20:00.932745Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"markdown","source":"## FastText","metadata":{}},{"cell_type":"code","source":"from multiprocessing import Pool\nfrom tqdm import tqdm_notebook as tqdm\n\nimport fasttext\nimport pymorphy2\nfrom string import punctuation\npunkt= [p for p in punctuation] + [\"`\", \"``\" ,\"''\", \"'\"]\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nstop = stopwords.words('russian')\n\nlemmatizer = pymorphy2.MorphAnalyzer(lang='ru')","metadata":{"execution":{"iopub.status.busy":"2022-06-12T23:32:49.885596Z","iopub.execute_input":"2022-06-12T23:32:49.886029Z","iopub.status.idle":"2022-06-12T23:32:50.602042Z","shell.execute_reply.started":"2022-06-12T23:32:49.885991Z","shell.execute_reply":"2022-06-12T23:32:50.601124Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#title_train, title_test, y_train, y_test = train_test_split(titles_preprocessed, y, test_size=0.2, stratify=y, random_state=42)\nft_train, ft_test = train_test_split(train, random_state=42, test_size=0.2, stratify = train['class'])","metadata":{"execution":{"iopub.status.busy":"2022-06-13T00:18:47.383788Z","iopub.execute_input":"2022-06-13T00:18:47.384202Z","iopub.status.idle":"2022-06-13T00:18:47.443044Z","shell.execute_reply.started":"2022-06-13T00:18:47.384157Z","shell.execute_reply":"2022-06-13T00:18:47.442351Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def to_ft_label(s):\n    return '__label__'+s.replace(',','_').replace(' ','_').replace('-','_')\n\nlabels_dict = {}\nfor g in train['class']:\n    labels_dict[to_ft_label(g)] = g","metadata":{"execution":{"iopub.status.busy":"2022-06-13T00:18:50.172827Z","iopub.execute_input":"2022-06-13T00:18:50.173339Z","iopub.status.idle":"2022-06-13T00:18:50.220768Z","shell.execute_reply.started":"2022-06-13T00:18:50.173282Z","shell.execute_reply":"2022-06-13T00:18:50.220024Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"from gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import remove_stopwords\ntrain.iloc[:, 0] = train.iloc[:, 0].apply(lambda x: ' '.join(simple_preprocess(x)))\ntest.iloc[:, 1] = test.iloc[:, 1].apply(lambda x: ' '.join(simple_preprocess(x)))","metadata":{"execution":{"iopub.status.busy":"2022-06-13T00:18:52.173556Z","iopub.execute_input":"2022-06-13T00:18:52.174095Z","iopub.status.idle":"2022-06-13T00:18:54.727863Z","shell.execute_reply.started":"2022-06-13T00:18:52.174052Z","shell.execute_reply":"2022-06-13T00:18:54.726983Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"col = ['class', 'text']\n\n# train\ntrain_for_ft = ft_train[col]\ntrain_for_ft['class']=[to_ft_label(s) for s in train_for_ft['class']]\n\n# test\ntest_for_ft = ft_test[col]\ntest_for_ft['class']=[to_ft_label(s) for s in test_for_ft['class']]","metadata":{"execution":{"iopub.status.busy":"2022-06-13T00:18:54.729653Z","iopub.execute_input":"2022-06-13T00:18:54.730068Z","iopub.status.idle":"2022-06-13T00:18:54.779866Z","shell.execute_reply.started":"2022-06-13T00:18:54.730028Z","shell.execute_reply":"2022-06-13T00:18:54.779063Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"train_for_ft.to_csv('train_for_ft.csv', index=False, sep=' ', header=False, escapechar=\" \")\ntest_for_ft.to_csv('test_for_ft.csv', index=False, sep=' ', header=False, escapechar=\" \")","metadata":{"execution":{"iopub.status.busy":"2022-06-13T00:18:55.432750Z","iopub.execute_input":"2022-06-13T00:18:55.433396Z","iopub.status.idle":"2022-06-13T00:18:55.806317Z","shell.execute_reply.started":"2022-06-13T00:18:55.433362Z","shell.execute_reply":"2022-06-13T00:18:55.805257Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"model = fasttext.train_supervised('train_for_ft.csv', lr = 0.9)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T00:18:57.042139Z","iopub.execute_input":"2022-06-13T00:18:57.042849Z","iopub.status.idle":"2022-06-13T00:18:59.293116Z","shell.execute_reply.started":"2022-06-13T00:18:57.042804Z","shell.execute_reply":"2022-06-13T00:18:59.292049Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"model.test('test_for_ft.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-13T00:19:03.662732Z","iopub.execute_input":"2022-06-13T00:19:03.663125Z","iopub.status.idle":"2022-06-13T00:19:03.841133Z","shell.execute_reply.started":"2022-06-13T00:19:03.663090Z","shell.execute_reply":"2022-06-13T00:19:03.840077Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def predict(test):\n    return labels_dict[ model.predict(test['text'], k=1)[0][0] ]\ntest['predictions'] = test.apply(predict,axis=1)\n\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T00:19:10.506894Z","iopub.execute_input":"2022-06-13T00:19:10.507265Z","iopub.status.idle":"2022-06-13T00:19:11.810122Z","shell.execute_reply.started":"2022-06-13T00:19:10.507232Z","shell.execute_reply":"2022-06-13T00:19:11.809171Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'id':range(0, len(test)),\n                           'class':test['predictions'].values},\n                          columns=['id', 'class'])\nsubmission.to_csv('submission1.csv', index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T23:47:15.453268Z","iopub.execute_input":"2022-06-12T23:47:15.453659Z","iopub.status.idle":"2022-06-12T23:47:15.509856Z","shell.execute_reply.started":"2022-06-12T23:47:15.453630Z","shell.execute_reply":"2022-06-12T23:47:15.509014Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"sub_predict = model.predict([text_sub_sequences, X_sub])\nsample_submission['class'] = sub_predict_nn2[:,0]\nsample_submission.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}